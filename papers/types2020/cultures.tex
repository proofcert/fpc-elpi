\section{Two cultures}

When studying structural proof theory, one learns quickly that many
concepts come in pairs: negative/positive, left/right,
bottom-up/top-down, premises/conclusion, introduction/elimination,
etc.  When we examine the larger setting of this project of linking a
logic programming engine with Coq and its kernel, we find a large
number of new pairing that is valuable to explicitly discuss.

\subsection{Proof theory vs type theory}

In many ways, proof theory is more elementary and low-level than most
approaches to type theory.  For example, type theories usually answer
the question ``What is a proof?'' with the response ``a (dependently)
typed $\lambda$-term''.  That is, when describing a type theory, one
usually decides that a proof is a certain kind of $\lambda$-term
within the system.  In contrast, the proof theory treats logical
propositions and proofs as separate.  For example, proof theory does
not assume that there are terms within the logic that describe proofs.

%% \begin{metanote}
%%   This paragraph seem to identify proof and type theory with sequents
%%   and nat ded and while I agree with the gist (although people such as
%%   Frank would argue that sequents are a way of organizing proof search
%%   for nat ded), we're not gonna make a lot of friends at types and
%%   more importantly is a big hammer to nail the point that logic
%%   programming is helpful with Coq --am
%% \end{metanote}

%% DM: I've tried to address your concerns by adding an additional
%% paragraph and references (below).

Gentzen's discovery that the key to treating classical and
intuitionistic logics in the same proof system was the invention of
the weakening and contraction structural rules on the right-side of
sequents~\cite{gentzen35}.  This invention lead him away from natural
deduction to multiple-conclusion sequent calculus, a very important
kind of proof that has not been given a satisfactory representation in
type theory.  This same innovation of Gentzen also opened the way to
another key proof-theoretic discovery, that of linear logic
\cite{girard87tcs}.  Similarly, sequent calculus provides an elegant
presentation of linear logic while most treatments of linear logic in
type theory have difficulty treating linear negation and the
multiplicative disjunction $\lpar$ and its unit $\bot$.

% DM I don't have a reference but I added more details.
%\begin{metanote}
%  `` partial fashion.'' Isn't that a bit vague? Some references? -am
%\end{metanote}

As is often observed, however, sequent calculus is too low-level to be
used explicitly as capturing the ``essence of a proof''.  Recent
developments in focused proof systems \cite{andreoli92jlc,liang09tcs},
makes it possible to extract from sequent calculus not only natural
deduction proofs \cite{pimentel16lsfa}, but also proof nets
\cite{chaudhuri08tcs}, and Herbrand-style expansion trees
\cite{chaudhuri16jlc}.  The role of focused proof systems to
characterize classes of proof structures is described in the next
section and used in our two example applications of Elpi with Coq.

%% DM I no longer think this comment is useful for this paper, so I
%% delete it instead of trying to fix it.

%% Proof theory also has rather immediate and natural treatments for
%% co-induction (as well as induction: see
%% \cite{baelde12tocl,momigliano12jal}) whereas the treatment of
%% coinduction within type theory remains more challenging and is well
%% developed only for various syntactic conditions (see, for example,
%% \cite{bertot08entcs}).
%% \begin{metanote}
%%   The criticism applies to Coq, less to type theory, see co-patterns --am
%% \end{metanote}

\subsection{Proof search vs proof normalization}

Gentzen-style proofs are used to model computation in at least two
different ways.  The functional programming paradigm, following the
Curry-Howard correspondence, models computation abstractly as 
$\beta$-reduction of natural deduction proof: that is, computation is
modeled as \emph{proof normalization}.  On the other hand, the
logic programming paradigm, following the notion of goal-directed
proof search, models computation abstractly as a regimented search of
cut-free sequent calculus proofs: that is, computation is modeled
as \emph{proof search}.

Proof search has features that are hard to capture in the proof
normalization setting.  In particular, Gentzen's \emph{eigenvariables}
are a kind of a proof-level binder.  In the proof normalization
setting, such eigenvariables are instantiated during proof
normalization.  However, during the search for cut-free proofs,
eigenvariables do not vary, and they are part of the syntax of terms
and formulas.  As a result, they can be used in the treatment of
bindings in data structures more generally.  Such eigenvariables can
be used by the logic programming paradigm to provide a natural,
elegant, and powerful computing method with bindings within syntax.

It is worth noting that the role of the cut rule and of
cut-elimination is different in these two approaches to computing.
In the proof normalization paradigm, the cut rule can be used to model
$\beta$ reduction, especially via the explicit substitution approach
\cite{lengrand06phd}. In the proof search paradigm, since computing involves
the search for cut-free proofs, the cut rule plays no part in the
performance of computation.  However, cut and cut-elimination can be
used to reason about computation: for example, cut-elimination can be
used to prove ``substitution lemmas for free'' \cite{gacek12jar}.

%% The following paragraph seems out-of-place so I delete it now.

%% In this paper, we shall use two different proof systems.  The first
%% logic is first-order intuitionistic logic.  Here, predicates
%% are not inductively defined and quantification will be limited to
%% range over first-order term.  Actually, we shall examine only the Horn
%% clause fragment of that logic.
%% %
%% The second logic will allow for inductively defined predicates using a
%% fixed point notation.   Given again our restriction to essentially
%% Horn clause specifications, it is possible to view intuitionistic
%% reasoning with such specification as if they are actually
%% specification in $\mu$MALL, an extension of multiplicative additive
%% linear logic with the least and greatest fixed point operators
%% \cite{baelde12tocl,baelde07lpar,heath19jar}.


\subsection{\lP vs Coq}

Given that \lP and Coq both result from a mixing of the
$\lambda$-calculus with logic, it is important to understand their
differences.  The confusion around the term \emph{higher-order
abstract syntax} (HOAS), for example, helps to illustrate their
differences.  In the functional programming setting, including the Coq
system, the HOAS approach leads to encoding binding structures within
terms using functions.  Such encodings are surprisingly complicated
since they allow for \emph{exotic terms} \cite{despeyroux95tlca} and
for structures on which induction is not possible
\cite{roeckl01fossacs}.  In the logic programming setting,
particularly \lP, HOAS is well supported since bindings are allowed
with terms ($\lambda$-bindings), in formulas (quantifiers), and in
proofs (eigenvariables).  For this reason, the term
\emph{$\lambda$-tree syntax} was introduced to name this particular
take on HOAS \cite{miller19jar}.  The Abella proof assistant
\cite{baelde14jfr} was designed, in part, to provide inductive and
co-inductive inference involving specifications using the
$\lambda$-tree syntax approach.

Another difference between \lP and functional programming can be
illustrated by considering how they are used in the specification of
tactics.  In fact, the origin story for the ML functional programming
language was that it was the meta-language for implementing the LCF
suite of tactics and tacticals \cite{gordon79}.  To implement tactics,
ML adopted not only novel features such as polymorphically typed
higher-order functional programming but also the non-functional
mechanisms of failure and exception handling.  While \lP is also based
on polymorphically typed higher-order relational programming, it also
comes with a completely declarative version of failure and
backtracking search.  Combining those features along with its support
of unification (even in the presence of term-level bindings), \lP
provides a rather different approach to the specification of tactics
\cite{felty93jar}.


While \lP is a typed language, it only uses simple, polymorphic types,
and these are more often used to indicate syntactic categories.  Any
dependency information needs to be captured using predicates and
quantifiers.
