\section{Two cultures}

When studying structural proof theory, one learns quickly that many
concepts come in pairs: negative/positive, left/right,
bottom-up/top-down, premises/conclusion, introduction/elimination,
etc.  When we examine the larger setting of this project of linking a
logic programming engine with Coq and its kernel, we find a large
number of new pairing that is valuable to explicitly discuss.

\subsection{Proof theory vs type theory}

In many ways, proof theory is more elementary and low-level than most
approaches to type theory.  For example, type theories usually answer
the question ``What is a proof?'' with the response ``a (dependently)
typed $\lambda$-term''.  That is, when describing a type theory, one
usually decides that a proof is a certain kind of $\lambda$-term
within the system.  In contrast, proof theory treats logical
propositions and proofs as separate.  For example, proof theory does
not assume that there are terms within the logic that describe proofs.

Gentzen's discovered that the key to treating classical and
intuitionistic logics in the same proof system was the weakening and
contraction structural rules on the right-side of
sequents~\cite{gentzen35}.  This discovery lead him away from natural
deduction to multiple-conclusion sequent calculus, a very important
kind of proof system that has not been given a satisfactory
representation in type theory.  This same innovation of Gentzen also
opened the way to another key proof-theoretic discovery, that of
linear logic \cite{girard87tcs}.  Similarly, sequent calculus provides
an elegant presentation of linear logic while most treatments of
linear logic in type theory have difficulty treating linear negation
and the multiplicative disjunction $\lpar$ and its unit $\bot$.

As is often observed, however, sequent calculus is too low-level to be
used explicitly as capturing the ``essence of a proof''.  Fortunately,
the notion of \emph{focused proof systems}
\cite{andreoli92jlc,liang09tcs}, makes it possible to collect and join
the micro-level inference rules of the sequent calculus into large
scale, synthetic inference rules.  As a result, using such proof
systems, it is possible to extract from sequent calculus not only
natural deduction proofs \cite{pimentel16lsfa}, but also proof nets
\cite{chaudhuri08tcs}, and Herbrand-style expansion trees
\cite{chaudhuri16jlc}.  The role of focused proof systems to
characterize classes of proof structures is described in the next
section and used in our two example applications of Elpi with Coq.

\subsection{Proof search vs proof normalization}

Gentzen-style proofs are used to model computation in at least two
different ways.  The functional programming paradigm, following the
Curry-Howard correspondence, models computation abstractly as
$\beta$-reduction of natural deduction proof: that is, computation is
modeled as \emph{proof normalization}.  On the other hand, the logic
programming paradigm, following the notion of goal-directed proof
search, models computation abstractly as a regimented search for
cut-free sequent calculus proofs: that is, computation is modeled as
\emph{proof search}.

Proof search has features that are hard to capture in the proof
normalization setting.  In particular, Gentzen's \emph{eigenvariables}
are a kind of proof-level binder.  In the proof normalization setting,
such eigenvariables are instantiated during proof normalization.
However, during the search for cut-free proofs, eigenvariables do not
vary, and they are part of the syntax of terms and formulas.  As a
result, they can be used in the treatment of bindings in data
structures more generally.  Such eigenvariables can be used by the
logic programming paradigm to provide a natural, elegant, and powerful
computing approach to bindings within syntax.

It is worth noting that the role of the cut rule and of
cut-elimination is different in these two approaches to computing.
In the proof normalization paradigm, the cut rule can be used to model
$\beta$ reduction, especially via the explicit substitution approach
\cite{lengrand06phd}. In the proof search paradigm, since computing involves
the search for cut-free proofs, the cut rule plays no part in the
performance of computation.  However, cut and cut-elimination can be
used to reason about computation: for example, cut-elimination can be
used to prove ``substitution lemmas for free'' that arise in the study
of operational semantics \cite{gacek12jar}.

\subsection{\lP vs Coq}

Given that \lP and Coq both result from combining the
$\lambda$-calculus with logic, it is important to understand some of
their their differences.  The confusion around the term
\emph{higher-order abstract syntax} (HOAS), is a case in point.  In
the functional programming setting, including the Coq system, the HOAS
approach leads to encoding binding structures within terms using
functions.  The earliest such encodings were unsatisfactory since they
would allow for \emph{exotic terms} \cite{despeyroux95tlca} and for
structures on which induction was not immediately possible \cite{roeckl01fossacs}.
Later approaches yielded non-canonical and complex encodings~\cite{chlipala08icfp,honsell01tcs}, as well as sophisticated type theories~\cite{pientka10ijcar}. All of
these could support inductive and coinductive reasoning. 
% DM I'm hoping that the additional sentence and references are enough
% here. 
%% \begin{metanote}
%%   AM: I find this a bit unbalanced. Exotic terms can be avoided with
%%   Honsell's theory of context and/or Adam's PHOAS -- not that I like
%%   it.. And of course there is Beluga and friends.  On the other hand
%%   Coq users are happy with library-based approaches such as Autosubt
%%   or well-typed DB that allows for sphisticated treatments of
%%   binders. I would say something more generic such as that mixing
%%   functions and HOAS is not immediate, may require additional
%%   machinery both in terms of type theories than of code generation,
%%   while in lp\dots
%% \end{metanote}
In the logic programming setting, particularly \lP, HOAS is well
supported since bindings are allowed with terms ($\lambda$-bindings),
in formulas (quantifiers), and in proofs (eigenvariables).  (In fact,
the original paper on HOAS \cite{pfenning88pldi} was inspired by \lP.)
For this reason, the term \emph{$\lambda$-tree syntax} was introduced
to name this particular take on HOAS \cite{miller19jar}.  The Abella
proof assistant \cite{baelde14jfr} was designed, in part, to provide
inductive and co-inductive inference involving specifications using
the $\lambda$-tree syntax approach.

Another difference between \lP and functional programming can be
illustrated by considering how they are used in the specification of
tactics.  The origin story for the ML functional programming language
was that it was the meta-language for implementing the LCF suite of
tactics and tacticals \cite{gordon79}.  To implement tactics, ML
adopted not only novel features such as polymorphically typed
higher-order functional programming but also the non-functional
mechanisms of failure and exception handling.  While \lP is also based
on ML-style-polymorphically typed higher-order relational
programming\footnote{It uses types to indicate syntactic categories, while
  any dependency information needs to be captured using predicates and
  logical connectives and quantifiers.}, it also comes with a
completely declarative version of failure and backtracking search.
Combining those features along with its support of unification (even
in the presence of term-level bindings), \lP provides a rather
different approach to the specification of tactics \cite{felty93jar}.

% While \lP is a typed language, it only uses ML-style polymorphic
% types, and these are more often 

% LocalWords:  Elpi polymorphically

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:
