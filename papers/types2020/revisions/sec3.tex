\section{Proof theory and proof certificates}
\label{sec:three}

In this section, we introduce the main ideas from \emph{focused proof
  system}, \emph{foundational proof certificates}, and the
\emph{Coq-Elpi} plugin that we need for this paper.

\subsection{Proofs for the Horn fragment}
\label{ssec:focused}

% \todo{Figure now uses Dale kernel-v2 --- needs further abstraction
% and explanation, see appendix} 

A \emph{Horn clause} is a formula of the form $\forall \bar
x_1. A_1\supset \forall \bar x_2. A_2\supset \forall \bar
x_n. A_n\supset A_0$ where $\forall\bar x_i$ denote a list of
universal quantifiers ($i\in\{1,\ldots,n\}$) and $A_0,\ldots,A_n$ are
atomic formulas.  It is well known that the following set of sequent
calculus proof rules are complete for both classical and
intuitionistic logic when one is attempting to prove that a given
atomic formula $A$ is provable from a  set $\Pscr$ of Horn
clauses. 
\[
  \infer[decide]{\Pscr\vdash A}
                {D \in \Pscr\quad \Pscr\Downarrow D\vdash A}
  \qquad
  \infer[init]{\Pscr\Downarrow A\vdash A}{}
\]
\[
  \infer[\forall L]{\Pscr\Downarrow\forall x. D\vdash A}
        {\Pscr\Downarrow D[t/x]\vdash A}
  \qquad
  \infer[\supset L]{\Pscr\Downarrow B\supset D\vdash A}
        {\Pscr\vdash B\qquad \Pscr\Downarrow D\vdash A}
\]
Here, we use two different styles of sequents.  The sequent
$\Pscr\vdash A$ is the usual sequent which we generally use as the end
sequent (the conclusion) of a proof.  The sequent $\Pscr\Downarrow
D\vdash A$ is a \emph{focused} sequent in which the formula $D$ is the
\emph{focus} of the sequent.  The two left introduction rules and the
initial rule can only be applied to the focused formula.  This latter
point is in contrast to Gentzen's sequent calculus where these rules
can involve \emph{any} formula on the left of the $\vdash$.  The fact
that this proof system is complete for both classical and
intuitionistic logic (when restricted to the Horn clause fragment)
follows from rather simple considerations of Horn clauses
\cite{nadathur90jacm} and from the completeness of LJT proofs
\cite{Herbelin94}.  The use of the term \emph{focus} comes from
Andreoli's proof system for linear logic \cite{andreoli92jlc}.

\begin{figure}[t]
\centering
  \[
\infer{\XXi\vdash k\ l : A}
      { \mathrm{Ind}[p] \ (\Gamma_I := \Gamma_C) \in E\quad 
       (\mathit{head} \ A) : T \in \Gamma_I\quad  
       k : D \in \Gamma_C\quad 
       \bc{\XXi_1}{} l D A \quad\unfoldExpert{\XXi}{\XXi_1}} 
\]
\vskip -18pt
\[
  \infer{\XXi\vdash  A : s}{E[] \vdash_{CIC} A : s\quad \sortExpert{\XXi}}% The order should be s : A, not A : s ?
  %% we are checking nat : Set, for example, so A : s --AM
  \qquad
  \infer{\bc{\XXi}{}{[]} A A'}{E[]\vdash_{CIC} A =_{\beta\delta\iota\xi}A'\quad\initLExpert{\XXi}}
\]
  \vskip -18pt
\[
  \infer{\bc{\XXi}{}{(t\ ::\ l)}{\depprod{x}{B}{D}}{A}}
        {\XXi_1\vdash t : B\quad
         \bc{\XXi_2}{}l {D[t/x]} A\quad
         \prodE{\XXi}{\XXi_1}{\XXi_2}{t}}
\]
\caption{Specification of a core calculus% of the checking of a proof certificate and the
  % synthesis of a dependently typed $\lambda$-term
  .}
\label{fig:augmented}
\end{figure}

\begin{figure}
\lstinputlisting[language=lprolog,linerange={sigs-end}]{../code/dep-kernel.sig}
\lstinputlisting[language=lprolog,linerange={check-end}]{../code/dep-kernel.mod}
\caption{Implementation of the core calculus.}
\label{fig:kernel}
\end{figure}

Figure~\ref{fig:augmented} contains an annotated version of these
proof rules: the annotations help us connect to elements of the Coq
proof system.
\begin{enumerate}
  \item Instead of having separate connectives for $\forall$ and
    $\supset$, we have the dependent product connective $(x:A)D$. 
  \item We account for \emph{computation} inside atoms by generalizing
    the \emph{init} rule to allow type-level conversion.
  \item We have incorporated  \emph{proof
  certificates}~\cite{chihani17jar} (using the schematic variable
    $\XXi$) along with \emph{expert predicates} (predicates with the $e$
    subscript). We explain these in Section~\ref{ssec:cert}.
  \item The inference rules are  annotated by terms structures that
    can be given directly to the Coq kernel for checking.
  \item We have added various premises which are responsible for
    interacting with Coq.
\end{enumerate}
The proof system described in Figure~\ref{fig:augmented} (and
implemented in Figure~\ref{fig:kernel}) corresponds to a subset of the Calculus
of Inductive Constructions in which inductive definitions are
limited to % first-order
Horn clauses.
%
This  system is inspired by the calculus for proof search in
Pure Type Systems introduced in \cite{LengrandDM06}, based in turn on
ideas stemming from focusing (in particular, the LJT calculus of
Herbelin \cite{Herbelin94}). Similar to that calculus, we have a
term language that includes terms and lists of terms, and two %distinct
typing judgments for the two categories.
% %
% In our case, the \lP goal formula \lsti{(check Xi (go B T))}
% corresponds to $\XXi\vdash B \colon T$ and the formula
% %
% \lsti{(check Xi (bc D B L))} corresponds to $\XXi\Downarrow L\colon
% D\vdash B$.
%
This style of  proof terms coincides with the idea behind the \emph{spine
  calculus}~\cite{Cervesato97tr}.  The main novelty of our proof
system here is that proof terms and proof certificates are used
simultaneously in all inference rules.

The proof system is parameterized by Coq's global environment $E$,
here a set of constant and inductive definitions; following Coq's
reference manual, inductive definition are denoted by
$\mathrm{Ind}[p] (\Gamma_I := \Gamma_C)$, where $\Gamma_I$ determines
the names and types of the (possibly mutually) inductive type and
$\Gamma_C$ the names and types of its constructors; finally $p$
denotes the number of parameters and plays here no role. The {local context} is empty, since we are only
dealing with types that correspond to Horn clauses, and atomic types
are inductively defined. In fact, we do not have a $\forall$ rule on
the right, although the proof theory would gladly allow it. This means
that there are no variables in our grammar of
terms. Terms are always applied to a (possibly empty) list of
arguments.   We delegate to
Coq's type checking the enforcement of the well-sortedness of
inductive types.  The {decide} rule, % is the equivalent in our
% a \emph{decide} rule in an explicitly focused system
% \cite{liang09tcs}.  
as in the previous proof system for Horn logic,  given an atom,
 selects a clause on which to backchain on: we lookup the
constructors of an inductive definition from the global
environment, one that matches the head symbol of the atom we aim to backchain on, and then
call the latter judgment that will find a correct instantiation, if any. The rules for backchaining include
 the (conflation of the) left introduction rules for
$\forall$ and $\supset$, as well as the \emph{init} rule, which incorporates Coq's conversion.

It may be at first surprising that there are no
introduction rules for propositional connectives, nor equality for
that matter. However, one of the beauties of the Calculus of Inductive
Construction is that they are, in fact, defined inductively and
therefore the \emph{decide} rule will handle those.  Thus, the syntax
of proof-terms is rather simple.

\subsection{Proof certificate checking}
\label{ssec:cert}
Figure~\ref{fig:kernel} contains the Elpi implementation of the
inference rules in Figure~\ref{fig:augmented}: $\XXi\vdash t \colon A$ corresponds to
\lsti{check Cert (go A T)} and $\XXi \Downarrow  l \colon D\vdash A$ corresponds to
\lsti{check Cert (bc D A L)}.
%
The code in that figure mixes both Coq-specific and
FPC-specific items.  We describe both of these separately.

\subsubsection{Coq-specific code}
Coq terms are accessed through the Coq-Elpi API, and their representation
in \lP takes advantage of native \lP constructs such as lists and binders.
The following is part of the Coq-Elpi API signature of constants that we use.
\begin{lstlisting}[language=lprolog]
kind term   type.                                   % reification of Coq terms
kind gref   type.                                   % reification of refs to globals
type global gref -> term.                            % coercion to term
type indt   inductive -> gref.                       % reification of inductive types
type indc   constructor -> gref.                     % reification of their constructors 
type app    list term -> term.                       % reification of nary application
type prod   name -> term -> (term -> term) -> term. % reification of dependent product
\end{lstlisting}
%
Note that \lsti{prod} encodes dependent products by taking a name for pretty
printing, a term and a \lP abstraction from terms to terms: i.e., $(x :
B) D$ is encoded by \verb|prod ``x'' B (x\ D x)|; when, in the
implementation of the product-left rule, we apply \lsti{D} to the variable
\lsti{Tm}, we get a new term that can be used to continue backchaining. This
application is obtained via meta-level substitution, in the style of HOAS. In
this sense, our calculus uses \emph{implicit} substitutions, rather than
explicit ones as in the  LJT and PTSC's tradition; this is consistent with proof search in
our application being cut-free, whereas explicit substitutions are linked to cuts.
% Syntax: one can think as if something of type \lsti{term -> term} were a
% \lsti{term} with a hole in it for another \lsti{term}.
The decide rule makes use of the Coq-Elpi primitives
\lsti{coq.safe-dest-app} to obtain the head term of a (possibly
nested) application, and \lsti{coq.env.indt} to access the global
environment of inductive definitions and query for information about them.
The \lsti{decideE} predicate,  has, among others, the role of selecting which
constructor to focus on from the inductive type. The kernel will successively
obtain the type of the selected constructor, and initiate the backchaining
phase. The latter is terminated when the focused atom unifies with the current goal, via
Elpi's primitive \lsti{coq.unify-eq}.


\subsubsection{FPC-specific code}
\label{ssec:fpc}

% DM I tried to down play FPC here so we would not need to say too
% much about the general picture.  Hopefully, the examples we provide
% and the few motivating sentences will make the reviewers happier.

The \emph{foundational proof certificates} (FPC) framework was
proposed in \cite{chihani17jar} as a flexible approach to specifying a
range of proof structures in first-order classical and intuitionistic
logics.  Such specifications are also executable using a simple logic
programming interpreter.  As a result of using logic programming,
proof certificates in this framework are allowed to lack details that
can be reconstructed during the checking phase.  For example,
substitution instances of quantifiers do not need to be explicitly
described within a certificate since unification within the logic
programming checker is often capable of reconstructing such
substitutions.

In this and the next section, we shall only use a much reduced subset
of the FPC framework: in essence, an FPC will be used as a simple
mechanism for bounding the search for proofs.  In our examples, a
proof certificate, denoted by the schematic variable $\XXi$, is a
particular term that is threaded throughout a logic programming
interperter.  For example, the inference rules in
Figure~\ref{fig:augmented} are augmented with an additional premise
that invokes an \emph{expert predicate} with is responsible for
extracting relevant information from a proof certificate $\XXi$ as
well as constructing continuation certificates, such as, $\XXi_1$ and
$\XXi_2$.  For example, the premise $\prodE{\XXi}{\XXi_1}{\XXi_2}{t}$
calls the expert for products which should extract from the
certificate $\XXi$ a substitution term $t$ and two continuation
certificates $\XXi_1$ and $\XXi_2$ for the two premises of this rule.
If the certificate $\XXi$ does not contain an explicit substitution
term, the expert predicate can simply return a logic variable which
would denote any term that satisfies subsequent unification problems
arising in completing the check of this certificate.

In our case here, an FPC is a collection of \lP clauses that provide
the remaining details not supplied in Figures~\ref{fig:augmented}
and~\ref{fig:kernel}: that is, the exact set of constructors for the
type of certificates \lsti{cert} as well as the specification of the
expert predicates listed \emph{ibidem}.  The top of
Figure~\ref{fig:resources} displays two FPCs, both of which can be
used to describe proofs where we bound the dimension of a proof.  For
example, the first FPC dictates that the query \mbox{\lsti{(check
    (qheight 5) A)}} is provable in the kernel using the clauses in
Figures~\ref{fig:kernel} and~\ref{fig:resources} if and only if the
height of that proof is 5 or less.  Similarly, the second FPC can be
used to bound the total number of instances of the decide rule in a
proof.  (Obviously, such proof certificates do not contain, for
example, substitution terms.)
%
%In particular, the query \mbox{\lsti{(check (qsize 5 H) B)}}
% \begin{lstlisting}
% sigma H\ (check (qsize 5 H) B)
% \end{lstlisting}
%is provable if and only if the total number of unfoldings of that
%proof is 5 or less.

\begin{figure}[t]
\lstinputlisting[language=lprolog,linerange={resources-end}]{../code/fpcs.sig}
\lstinputlisting[language=lprolog,linerange={resources-end}]{../code/fpcs.mod}
\caption{Sample FPCs}
\label{fig:resources}
\end{figure}

As it has been described in \cite{blanco17cade}, it is also possible
to pair together two different proof certificates, defined by two
different FPC definitions, and do the proof checking in parallel.
This means that we can build an FPC that \emph{restricts} proofs
satisfying two FPCs simultaneously.  In particular, the infix
constructor \lsti{<c>} in Figure~\ref{fig:resources} forms the pair of
two proof certificates and the pairing experts for the certificate
\lsti{Cert1 <c> Cert2} simply request that the corresponding experts
also succeed for both \lsti{Cert1} and \lsti{Cert2}. Thus, the query
\verb+check ((qheight 4) <c> (qsize 10)) A+ will succeed if there is a
proof of \lsti{A} that has a height less than or equal to 4 while also
being of size less than or equal to 10.

%%\todo{MM: Talk about randomized FPC? They mentioned afterwards -am}


\subsection{A Prolog-like tactic}

Thanks to the Coq-Elpi interface, in particular to the ``main''
procedure \lsti{solve}, we can package the \lP code for the checker
as a tactic that can be called as any other tactic in a Coq script.

\begin{lstlisting}[deletekeywords={goal}]
Elpi Tactic dprolog.
Elpi Accumulate lp:{{
  solve [str ''height'', int N] [goal _  Ev G _] _ :-
    coq.say "Goal:" {coq.term->string G},
    check (qheight N) (go G Term),
    Ev = Term,
    coq.say "Proof:" {coq.term->string Ev}.
... (* Other clauses for different fpc omitted *)
}}.
\end{lstlisting}

The glue code between Coq-Elpi and the implementation of our calculus
is straightforward: the goal consists of a quadruple of a (here
inactive) context, an \emph{evar}, a type (goal) and a list of extra
information, also inactive. In addition, we supply the certificate: it
consists of an integer (or two in the case of pairing) and a string to
identify the ``resource'' FPC that we will use in this case. We just
need to call \lsti{check} on the goal \lsti{G}, together with the
certificate, in order to obtain a reconstructed proof term. We do not
call the reconstruction directly on the evar because Coq-Elpi ensures
that evars manipulated by \lP are well-typed at all times; since we
cannot guarantee that, as we work with partially reconstructed term,
we get around this by an explicit unification.

The following
example shows how we can use the above tactic to do FPC-driven logic
programming modulo conversion in Coq and return a Coq proof-term:
\begin{lstlisting}[]
Inductive insert (x:nat) : list nat -> list nat -> Prop :=
| i_n : insert x [] [x]
| i_s : forall y: nat, forall ys, x <= y -> insert x (y :: ys) (x :: y :: ys)
| i_c : forall y: nat, forall ys rs, y <= x -> insert x ys rs -> insert x (y :: ys) (x :: rs).
Lemma i1:  exists R, insert 2 ([0] $++$ [1]) R.
elpi dprolog height 10.
Qed.
Print i1.
ex_intro (fun R : list nat => insert 2 ([0] $++$ [1]) R)   [0; 1; 2]
  (i_c 2 0 [1] [1; 2] (le_S 0 1 (le_S 0 0 (le_n 0)))
	 (i_c 2 1 [] [2] (le_S 1 1 (le_n 1)) (i_n 2)))
\end{lstlisting}

The \lsti{dprolog} tactic can be seen as a programmable (and possibly
faster) version of Coq's \lsti{eauto}% , although it currently lacks the
% latter's capability of solving goals by
% \lsti{reflexivity}
; % as well as the modularity of using
% \lsti{Hints} as introduction rules
 \lsti{dprolog} is not restricted to depth-first search, since
it follows the dictates of the given FPC; for example we could easily add
iterative-deepening search. Furthermore,
%% commented out till we know for sure:
%%the tactic can still use\lsti{Hints} via Elpi's \lsti{Db} feature, and
 FPCs can provide a
\emph{trace} that may be more customizable than the one offered by 
\texttt{(e)auto}'s hard-wired \texttt{Debug} facility. 

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:

% LocalWords:  backchaining disjunct LJT dep qheight qsize orE someE
% LocalWords:  Elpi modularity Herbelin bc PTSC's dprolog CIC init lp
% LocalWords:  Andreoli's basicstyle lprolog sigs Coq's sortedness Ev
% LocalWords:  backchain gref indt indc coq dest unfoldings str fpc
% LocalWords:  evar nat forall ys elpi Qed lc args ljf HOAS unfoldE
% LocalWords:  FPCs Prolog evars untyped decideE eauto
