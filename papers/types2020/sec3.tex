\section{Proof theory and proof certificates}
\label{sec:three}

In this section, we introduce the main ideas from \emph{focused proof
  system}, \emph{foundational proof certificates}, and the
\emph{Coq-Elpi} plugin.

\subsection{Proof for the Horn fragment}
\label{ssec:focused}

% \todo{Figure now uses Dale kernel-v2 --- needs further abstraction
% and explanation, see appendix} 

A \emph{Horn clause} is a formula of the form $\forall \bar
x_1. A_1\supset \forall \bar x_2. A_2\supset \forall \bar
x_n. A_n\supset A_0$ where $\forall\bar x_i$ denote a list of
universal quantifiers ($i\in\{1,\ldots,n\}$) and $A_0,\ldots,A_n$ are
atomic formulas.  It is well known that the following set of sequent
calculus proof rules are complete for both classical and
intuitionistic logic when one is attempting to prove that a given
atomic formula $A$ is provable from a give set $\Pscr$ of Horn
clauses. 
\[
  \infer[decide]{\Pscr\vdash A}
                {D \in \Pscr\quad \Pscr\Downarrow D\vdash A}
  \qquad
  \infer[init]{\Pscr\Downarrow A\vdash A}{}
\]
\[
  \infer[\forall L]{\Pscr\Downarrow\forall x. D\vdash A}
        {\Pscr\Downarrow D[t/x]\vdash A}
  \qquad
  \infer[\supset L]{\Pscr\Downarrow B\supset D\vdash A}
        {\Pscr\vdash B\qquad \Pscr\Downarrow D\vdash A}
\]
Here, we use two different styles of sequents.  The sequent
$\Pscr\vdash A$ is the usual sequent which we generally use as the end
sequent (the conclusion) of a proof.  The sequent $\Pscr\Downarrow
D\vdash A$ is a \emph{focused} sequent in which the formula $D$ is the
\emph{focus} of the sequent.  The two left introduction rules and the
initial rule can only be applied to the focused formula.  This latter
point is in contrast to Gentzen's sequent calculus where these rules
can involve \emph{any} formula on the left of the $\vdash$.  The fact
that this proof system is complete for both classical and
intuitionistic logic (when restricted to the Horn clause fragment)
follows from rather simple considerations of Horn clauses
\cite{nadathur90jacm} and from the completeness of LJT proofs
\cite{Herbelin94}.  The use of the term \emph{focus} comes from
Andreoli's proof system for linear logic \cite{andreoli92jlc}.

\begin{figure}[t]
\[
\infer{\XXi_1\vdash k\ l : A}
      { \mathrm{Ind}[p] \ (\Gamma_I := \Gamma_C) \in E\quad 
       (\mathit{head} \ A : T)\in \Gamma_I\quad  
       k : D \in \Gamma_C\quad 
       \bc{\XXi_2}{} l D A \quad\unfoldExpert{\XXi_1}{\XXi_2}} 
\]
\vskip -18pt
\[
  \infer{\XXi\vdash  s : A}{{E[] \vdash_{CIC} s : A}}% The order should be s : A, not A : s ?
  \qquad
  \infer{\bc{\XXi}{}{[]} A A}{\initLExpert{\XXi}}
  \qquad
  \infer{\bc{\XXi}{}{(t\ ::\ l)}{\depprod{x}{B}{D}}{A}}
        {\XXi_1\vdash t : B\qquad
         \bc{\XXi_2}{}l {D[t/x]} A\qquad
         \prodE{\XXi}{\XXi_1}{\XXi_2}{t}}
\]
\caption{Specification of the checking of a proof certificate and the
  synthesis of a dependently typed $\lambda$-term.}
\label{fig:augmented}
\end{figure}

\begin{figure}
\lstinputlisting[basicstyle=\ttfamily,language=lprolog,linerange={sigs-end}]{code/dep-kernel.sig}
\lstinputlisting[basicstyle=\ttfamily,language=lprolog,linerange={check-end}]{code/dep-kernel-v2.mod}
\caption{Implementation of a  dependent kernel for Coq terms.}
\label{fig:kernel}
\end{figure}

Figure~\ref{fig:augmented} contains these proof rules and extends them
in the following fours ways so that it more appropriately fits into
the Coq proof system.
\begin{enumerate}
  \item Instead of having separate connectives for $\forall$ and
    $\supset$, we have the dependent product connective $(x:A)D$. 
  \item We have incorporated both \emph{proof certificates} (using the
    schematic variable $\XXi$) along with expert predicates
    (predicates with the $e$ subscript).
  \item The inference rules are also annotated by terms structures that
    can be give directly to the Coq kernel for checking.
  \item We have added various premises which are responsible to
    pulling information across the Elpi API with Coq.
\end{enumerate}
The proof system described in Figure~\ref{fig:kernel} corresponds to
the Calculus of Inductive Constructions in which all inductive
definitions are limited to the Horn clause.
%
This proof system is inspired by the calculus for proof search in
Pure Type Systems introduced in \cite{LengrandDM06}, based in turn on
ideas stemming from focusing (in particular, the LJT calculus of
Herbelin \cite{Herbelin94}). Similar to that calculus, we have a
term language that includes terms and lists of terms, and two distinct
typing judgments for the two categories.
%
In our case, the \lP goal formula \lsti{(check Xi (go B T))}
corresponds to $\XXi\vdash B \colon T$ and the formula
%
\lsti{(check Xi (bc D B L))} corresponds to $\XXi\Downarrow L\colon
D\vdash B$.
%
This style of encode coincides with the idea behind the \emph{spine
  calculus}~\cite{Cervesato97tr}.  The main novelty of our proof
system here is that proof terms and proof certificates are used
simultaneously in all inference rules.

The proof system is parameterized by Coq's global environment $E$, which
here can be thought to consist of a set of inductive definitions,
which following Coq's reference manual we denote with
$\mathrm{Ind}[p]  (\Gamma_I := \Gamma_C)$.  One  rule delegates
to Coq's type checking enforcing the well-sortedness of such types. We
do not have a \emph{local context} since we are only dealing with types
that correspond to Horn clauses, and atomic types are inductively
defined. In fact, we do not have a $\forall$ rule on the right,
although the proof theory would gladly allow it. This means, at the
term level, that we do not have variables in our grammar of
terms. Terms are always applied to a (possibly empty) list of
arguments.

The \emph{unfold} rule is the equivalent in our simplified setting of
a \emph{decide} rule in an explicitly focused system
\cite{liang09tcs}.  From a logic programming viewpoint, given an atom,
the rule selects a clause on which to back-chain on: we look-up the
constructors from an \lsti{Inductive} definition from the global
environment, one that matches the atom we wish to backchain on and then
call the latter judgment. The rules for backchaining are standard,
corresponding to the \emph{init} and the left introduction rules for
$\forall$ and $\supset$.

It may be at first surprising that there are no
introduction rules for propositional connectives, nor equality for
that matter. However, one of the beauty of the Calculus of Inductive
Construction is that they are, in fact,  defined inductively and
therefore the \emph{unfold} rule will handle those; this also
simplifies the syntax of proof-terms.

\subsection{Proof certificate checking}

Figure~\ref{fig:kernel} contains the Elpi implementation of the
inference rules in Figure~\ref{fig:augmented}: $\XXi\vdash t \colon A$ corresponds to
\lsti{check $\XXi$ (go A T)} and $\XXi \Downarrow  l \colon D\vdash A$ corresponds to
\lsti{check $\XXi$ (bc D A L)}.

% here the infix
% turnstile $\vdash$ symbol is replaced by the \lsti{check} predicate, and every rule
% is directly mapped to a clause in the code. The argument of the clause is
% either given by a \lsti{go} or a \lsti{bc} constructor, signaling whether the clause
% is a goal-reduction or a backchaining one.

Coq terms are accessed through the Coq-Elpi API, and their representation
in \lP takes advantage of the native \lP constructs such as lists and binders.
The following is a list of some of the signatures of the notions that we use:
\begin{lstlisting}[language=lprolog]
kind term   type.                                   % reification of Coq terms
kind gref   type.                                   % reif. of references to global terms
type global gref -> term.                            % coercion to term
type indt   inductive -> gref.                       % reification of inductive types
type indc   constructor -> gref.                     % reification of their constructors 
type app    list term -> term.                       % reification of nary application
type prod   name -> term -> (term -> term) -> term. % reification of dependent product
\end{lstlisting}

Note that \lsti{prod} encodes dependent products taking a name for pretty
printing, a term and a \lP function from terms to terms: namely $\forall x :
A, B$ is encoded by \lsti{prod ``x'' A (x\ B x)}; when, in the
implementation of the product-left rule, we apply $B$ to the variable
\lsti{Tm}, we get a term that can be used to continue backchaining. This
application is obtained via meta-level substitution, in the style of HOAS. In
this sense, our calculus uses \emph{implicit} substitutions, rather than
explicit ones as in LJT and PTSC's tradition, the more since proof search in
our application is cut-free.
% Syntax: one can think as if something of type \lsti{term -> term} were a
% \lsti{term} with a hole in it for another \lsti{term}.
The unfolding rule makes use of the Coq-Elpi primitives
\lsti{coq.safe-dest-app} in order to obtain the head term of a (possibly
nested) application, and \lsti{coq.env.indt} in order to access the global
environment of inductive definitions and query for information about them.
The \lsti{unfold_expert} predicate, then, has the role of selecting which
constructor to introduce for the inductive type. The kernel will successively
obtain the type of the selected constructor, and initiate the backchaining
phase on it.

In Figure~\ref{fig:augmented}  each
inference rule is augmented with an additional premise involving an
\emph{expert predicate}, a certificate $\Xi$, and possibly continuations of
certificates ($\Xi'$, $\Xi_1$, $\Xi_2$) with extracted information from
These notions are taken from the general setting of \emph{foundational
  proof certificates}~\cite{chihani17jar}.
%
\todo{MM: Talk about randomized FPC?}
In our case here, an FPC is a collection of \lP clauses that
provide the remaining details not supplied in Figure~\ref{fig:augmented} and~\ref{fig:kernel}:
that is, the exact set of constructors for the  type of certificates as
well as the specification of the expert predicates listed \emph{ibidem}.
%
The top of Figure~\ref{fig:resources} displays two FPCs,
both of which can be used to describe proofs where we bound
the dimension of  a proof.
%
For example, the first FPC dictates that the query \mbox{\lsti{(check (qheight 5) B)}} is
provable in  the kernel using the clauses in Figures~\ref{fig:kernel}
and~\ref{fig:resources} if and only if the height of that proof is 5
or less.
%
Similarly, the second FPC can be used to bound the total number of instances of
unfoldings in a proof.
%
%In particular, the query \mbox{\lsti{(check (qsize 5 H) B)}}
% \begin{lstlisting}
% sigma H\ (check (qsize 5 H) B)
% \end{lstlisting}
%is provable if and only if the total number of unfoldings of that
%proof is 5 or less.

\begin{figure}[t]
\lstinputlisting[basicstyle=\ttfamily,language=lprolog,linerange={resources-end}]{code/fpcs.sig}
\lstinputlisting[basicstyle=\ttfamily,language=lprolog,linerange={resources-end}]{code/fpcs.mod}
\caption{Sample FPCs}
\label{fig:resources}
\end{figure}

\todo{We could move here from elab the paragraph ``It is also possible to pair two different proof certificates, defined
by two different FPC definitions,'' and/or reduce redundancy}
Further, if we view a particular FPC as a means of restricting proofs, it is
possible to build an FPC that \emph{restricts} proofs satisfying two FPCs
simultaneously.
%
% In particular, Figure~\ref{fig:pairing} defines an FPC based on the
% (infix) constructor \lsti{<c>}, which \emph{pairs} two terms of type
% \lsti{cert}.
%
The pairing experts for the certificate \lsti{Cert1 <c> Cert2} simply
request that the corresponding experts also succeed for both
\lsti{Cert1} and \lsti{Cert2} 
%
Thus, the query \lsti{check ((qheight 4) <c> (qsize 10)) B}
will succeed if there is a proof of \lsti{B} that has a height less
than or equal to 4 while also being of size less than or equal to 10.

% Various  additional examples and experiments using the pairing of FPCs
% can be found in~\cite{blanco17cade}. Using similar techniques, it is possible to define FPCs that target
% specific types for special treatment: for example, when generating
% integers, only (user-defined) small integers could be produced.


\subsection{A prolog-like tactic}

Thanks to the Coq-Elpi interface, in particular to the ``main''
procedure \lsti{solve}, we can package the \lP's code for the checker
as a tactic that can be called as any other tactic in a Coq script.

\begin{lstlisting}[deletekeywords={goal}]
Elpi Tactic dprolog.
Elpi Accumulate lp:{{
  solve [str ''height'', int N] [goal _  Ev G _] _ :-
    coq.say "Goal:" {coq.term->string G},
    check (qheight N) (go G Term),
    Ev = Term,
    coq.say "Proof:" {coq.term->string Ev}.
...
(* Other clauses for different fpc omitted *)
}}.
\end{lstlisting}

The glue code between Coq-Elpi and the implementation of our calculus is
straightforward: the goal is given to us as a quadruple of a context, an
evar, a type and a list of extra information. In addition, the certificate is
supplied by the user when calling the tactic (it simply is an integer in this
case). We just need to call \lsti{check} on the goal's type, together with
the certificate, in order to obtain a reconstructed proof term. We avoid
calling the reconstruction directly on the evar because Coq-Elpi ensures that
evars manipulated by \lP are well-typed at all times, and we cannot guarantee
that since we work with partially reconstructed term; getting around this is
simply a matter of assigning the reconstructed term to the evar right
afterwards.
The following example shows how we can use the above tactic to do
FPC-driven logic programming in Coq and return a Coq proof-term:
\begin{lstlisting}
Inductive insert (x:nat) : list nat -> list nat -> Prop :=
| i_n : is_nat x -> insert x [] [x]
| i_s : forall y: nat, forall ys, x <= y -> insert x (y :: ys) (x :: y :: ys)
| i_c : forall y: nat, forall ys rs, y <= x -> insert x ys rs -> insert x (y :: ys) (x :: rs).
Lemma i1:  exists R, insert 2 [0;1] R.
eexists.
elpi dprolog height 10.
Qed.
Print i1.
ex_intro (fun R : list nat => insert 2 [0; 1] R) [0; 1; 2]
  (i_c 2 0 [1] [1; 2] (i_c 2 1 [] [2] (i_n 2)) : exists R : list nat, insert 2 [0; 1] R
\end{lstlisting}

The \lsti{dprolog} tactic can be seen as a programmable version of
Coq's \lsti{auto}: it currently lacks the latter's capability of
solving goals by \lsti{reflexivity} as well as the modularity of using
\lsti{Hints} as introduction rules; on the bright side, it is not
restricted to depth-first search, since it follows the dictate of the
given FPC, for example iterative-deepening. Furthermore, the FPC can
provide a \emph{trace} that may be more customized than the one
offered by the \lsti{auto}'s hard-wired \lsti{Debug} facility.

\subsection{Moving beyond Horn clauses}
\label{ssec:beyond}

If we do not restrict our attention to only Horn clauses then there is
no problem will presenting a focused proof system for all of
first-order intuitionistic logic using the connectives $\forall$ and
$\supset$. The following rules extend those given earlier.
\[
  \infer[decide]{\Pscr\vdash A}{D \in \Pscr\quad \Pscr\Downarrow D\vdash A}
  \qquad
  \infer[init]{\Pscr\Downarrow A\vdash A}{}
\]
\[
  \infer[\forall L]{\Pscr\Downarrow\forall x. D\vdash A}
        {\Pscr\Downarrow D[t/x]\vdash A}
  \qquad
  \infer[\supset L]{\Pscr\Downarrow B\supset D\vdash A}
        {\Pscr\vdash B\quad \Pscr\Downarrow D\vdash A}
  \qquad
  \infer[\supset R]{\Pscr\vdash B\supset D}{\Pscr, B\vdash D}
  \qquad
  \infer[\forall R]{\Pscr\vdash \forall x.D}{\Pscr\vdash D[y/x]}
\]
As before, the syntactic variable $A$ denotes atomic formulas and, as
usual, the $\forall R$ rule has the restriction that $y$ is not free in
its conclusion.

As has been detailed in earlier work on foundational proof
certificates, this richer notion of proof system can provide for
richer proof certificates.  The main differences with what we have
seen before is that the left-hand sides of sequents can now grow
during the proof checking process.  As a result, an indexing mechanism
is used so that it is possible for the decide expert to properly
select locally added assumptions instead of just those assumptions
accepted globally.

To illustrate a proof certificate format that uses this richer logic
and proof system, consider the simple \lP code in
Figure~\ref{fig:debruijn}.  Using the constants provided in that
figure, the untyped $\lambda$-term
$\lambda x (x (\lambda y (y (\lambda z (x (\lambda u\,z))))))$ can be
encoded as the following Elpi term of type \lsti{deb}.
%
\begin{lstlisting}[basicstyle=\ttfamily,language=lprolog]
(lambda (apply 0 [lambda (apply 0 [lambda (apply 2 
                                                    [lambda (apply 1 [])])])]))
\end{lstlisting}
Using the constructor \lsti{lc} and \lsti{args}, terms in De Bruijn
syntax (terms of type \lsti{deb}) are incorporated into proof
certificates (terms of type \lsti{cert}) along with other integer
arguments that are needed to compute offsets to address bound
variables.  
%
This FPC can be interpreted by the proof certificate checker in the
file \verb+ljf-dep.mod+ of the repository
\url{https://github.com/proofcert/fpc-elpi/}.  We note that this proof
certificate checker is also able to simultaneously synthesize a Coq
proof term while checking (see Section~\ref{sec:elab}) all the while
remaining less than 90 lines of Elpi code.

\begin{figure}
\lstinputlisting[basicstyle=\ttfamily,language=lprolog,linerange={start-end}]{code/deb-fpc.sig}
\lstinputlisting[basicstyle=\ttfamily,language=lprolog,linerange={start-end}]{code/deb-fpc.mod}
\caption{The FPC definition of De Bruijn notation as proof evidence.}
\label{fig:debruijn}
\end{figure}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:

% LocalWords:  backchaining disjunct LJT dep qheight qsize orE someE
% LocalWords:  Elpi modularity Herbelin bc PTSC's dprolog CIC init lp
% LocalWords:  Andreoli's basicstyle lprolog sigs Coq's sortedness Ev
% LocalWords:  backchain gref indt indc coq dest unfoldings str fpc
% LocalWords:  evar nat forall ys elpi Qed lc args ljf
