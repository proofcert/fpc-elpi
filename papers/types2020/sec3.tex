
\newpage
\section{Proof theory and proof certificates}
\label{sec:three}

\newcommand{\XXi}{{\color{blue}{\Xi}}}
\newcommand{\bc}[5]{#1#2\Downarrow #3 : #4 \vdash #5}

\todo{Right now there is no intro to FPC}
% \begin{metanote}
%   (AM): Start with the logical reconstruction of \texttt{check}, to and then reverse engineer what we want/need to say about focusing and FPC. We \textbf{do} not talk about PBT yet, just logic programming (with proof terms). % In sec 4 we package \texttt{check} in a PBT query and explain how \texttt{check} works for term generation.
% \end{metanote}

\subsection{Proof systems with certificates}
\label{ssec:focused}
\todo{Change title? }

% The proof search approach to encoding Horn clause computation
% results in the structuring of proofs with repeated
% switching between a \emph{goal-reduction} phase and a
% \emph{backchaining} phase~\cite{miller91apal}.

%\subsection{Certificate checking with expert predicates}
%\subsection{Adding certificates and experts to a proof system}
%\label{ssec:fpc}

% \todo{Figure now uses Dale kernel-v2 --- needs further abstraction and explanation, see appendix}

\begin{figure}[t]
\[
\infer{\XXi_1\vdash k\ l : A}
      { \mathrm{Ind}[p] \ (\Gamma_I := \Gamma_C) \in E\quad 
       (\mathit{head} \ A : T)\in \Gamma_I\quad  
       k : D \in \Gamma_C\quad 
       \bc{\XXi_2}{} l D A \quad\unfoldExpert{\XXi_1}{\XXi_2}} 
\]
\vskip -18pt
\[
  \infer{\XXi\vdash  s : A}{{E[] \vdash_{CIC} s : A}}% The order should be s : A, not A : s ?
  \qquad
  \infer{\bc{\XXi}{}{[]} A A}{\trueExpert{\XXi}}
  \qquad
  \infer{\bc{\XXi}{}{(t\ ::\ l)}{\forall x:B.\, D} A}
        {\XXi_1\vdash t : B\qquad
         \bc{\XXi_2}{}l {D[t/x]} A\qquad
         \prodExpert{\XXi}{\XXi_1}{\XXi_2}}
\]
\caption{Specification of the checking of a proof certificate and the
  synthesis of a dependently typed $\lambda$-term.}
\label{fig:augmented}
\end{figure}

\[
  \infer{\Pscr\vdash A}{D \in \Pscr\quad \Pscr\Downarrow D\vdash A}
  \qquad
  \infer{\Pscr\Downarrow A\vdash A}{}
  \qquad
  \infer{\Pscr\Downarrow\forall x. B(x)\supset D(x)\vdash A}
        {\vdash B(t)\qquad \Pscr\Downarrow D(t)\vdash A}
\]
Here, $\Pscr$ is a collection of Horn clauses, i.e., formulas of the
form $\forall \bar x_1. A_1\supset \forall \bar x_2. A_2\supset
\forall \bar x_n. A_n\supset A_0$. 

\begin{figure}
\lstinputlisting[basicstyle=\ttfamily,language=lprolog,linerange={sigs-end}]{code/dep-kernel.sig}
\lstinputlisting[basicstyle=\ttfamily,language=lprolog,linerange={check-end}]{code/dep-kernel-v2.mod}
\caption{Implementation of a  dependent kernel for Coq terms.}
\label{fig:kernel}
\end{figure}


Figure~\ref{fig:augmented} contains an implicitly focused proof system  for a small fragment of the Calculus of Inductive Constructions that corresponds to Horn logic programming over pure \lsti{Inductive} definitions.
%
It is a sequent calculus inspired by the calculus for
proof search in Pure Type Systems introduced in \cite{LengrandDM06}, based in
 turn on ideas stemming from focusing (in particular, the LJT calculus of
Herbelin \cite{Herbelin94}). Similarly to that calculus, we have a term
language that includes terms and lists of terms, and two distinct typing
judgments for the two categories. In our case, where proof search steps are
viewed as computation, the term judgment $\XXi\vdash t \colon A$ corresponds to a goal-reduction
step while the judgment $\XXi \Downarrow  l \colon D\vdash A$ corresponds to a backchaining step. This coincides
with the idea behind the \emph{spine calculus}~\cite{Cervesato97tr}.


The proof system is parameterized by Coq' global environment $E$, which
here can be thought to consist of a set of inductive definitions,
which following Coq's reference manual we denote with
$\mathrm{Ind}[p]  (\Gamma_I := \Gamma_C)$.  One  rule delegates
to Coq's type checking enforcing the well-sortedness of such types. We
do not have a \emph{local context} since we are only dealing with types
that correspond to Horn clauses, and atomic types are inductively
defined. In fact, we do not have a $\forall$ rule on the right,
although the proof theory would gladly allow it. This means, at the
term level, that we do not have variables in our grammar of
terms. Terms are always applied to a (possibly empty) list of
arguments.

The \emph{unfold} rule is the equivalent in our simplified setting of
a \emph{decide} rule in an explicitly focused system. From a logic
programming viewpoint, given an atom, the rule selects a clause on which to
back-chain on: we look-up the costructors from an \lsti{Inductive}
definition from the global enviroment, one that matches the atom we wish to bachchain on and then call the latter judgment. The rules for backchaining are standard, corresponding to the \emph{init} and $\forall$-left rules.

It may be at first surprising that there are no
introduction rules for propositional connectives, nor equality for
that matter. However, one of the beauty of the Calculus of Inductive
Construction is that they are in fact defined inductively and
therefore the \emph{unfold} rule will handle those; this also
simplifies the syntax of proof-terms.

  % , and as is usual in Pure Type
% Systems we have the same grammar for terms and types
%
%, as shown in figure~\ref{fig:terms}.
% \todo{Let's think about that: we need A at least, but A is also a metavariable for atoms}
% %\begin{figure}
% \begin{align*}
% t,u ::= & k\ l \;|\; \forall x : t.\, u \\
% l ::= & [] \;|\; (t\ ::\ l)
% \end{align*}
% % \caption{Grammar of terms}
% % \label{fig:terms}
% % \end{figure}


% we denote by $\Pscr_A$ the
% set of type declarations for the constructors of type $A$, of the form
% $k : D$ where $D$ is of the form
% $\forall x_1 : t_1.\, \dots \forall x_n : t_n.\, D\ u_1 \dots u_m$. We
% denote by $\mathcal{A}$ the set of arity declaration for the inductive
% types.

%
%
%done Explain relations with other term assignments. Note that we do not use
% explicit substitutions (we are cut free)
% Explain the choice of not having certificates represent proof terms.
% \todo{  Explain that the unfold clause handles
% defined atoms and defined connectives (conjunction, disjunction, existential
% and also equality): this may be less efficient and proof-theoretically induce
% pointless phase changes, but it is arguably very elegant e simplifies proof
% terms that are inherited by Coq }

Figure~\ref{fig:kernel} contains the Elpi implementation of the
inference rules in Figure~\ref{fig:augmented}: $\XXi\vdash t \colon A$ corresponds to
\lsti{check $\XXi$ (go A T)} and $\XXi \Downarrow  l \colon D\vdash A$ corresponds to
\lsti{check $\XXi$ (bc D A L)}.

% here the infix
% turnstile $\vdash$ symbol is replaced by the \lsti{check} predicate, and every rule
% is directly mapped to a clause in the code. The argument of the clause is
% either given by a \lsti{go} or a \lsti{bc} constructor, signaling whether the clause
% is a goal-reduction or a backchaining one.

Coq terms are accessed through the Coq-Elpi API, and their representation
in \lP takes advantage of the native \lP constructs such as lists and binders.
The following is a list of some of the signatures of the notions that we use:
\begin{lstlisting}
  kind term   type.                                   % reification of Coq terms
  kind gref    type                                 % reification of refernces to global terms
  type global gref -> term.                      % coercion to term
  type indt   inductive -> gref.                 % reification of inductive types
  type indc   constructor -> gref.             % reification of their constructors 
  type app    list term -> term.               % reification of nary application
  type prod   name -> term -> (term -> term) -> term. % reification of dependent product
\end{lstlisting}

Note that \lsti{prod} encodes dependent products taking a name for pretty
printing, a term and a \lP function from terms to terms: namely $\forall x :
A, B$ is encoded by \texttt{prod ``x'' A (x\textbackslash{} B x)}; when, in the
implementation of the product-left rule, we apply $B$ to the variable
\lsti{Tm}, we get a term that can be used to continue backchaining. This
application is obtained via meta-level substitution, in the style of HOAS. In
this sense, our calculus uses \emph{implicit} substitutions, rather than
explicit ones as in LJT and PTSC's tradition, the more since proof search in
our application is cut-free.
% Syntax: one can think as if something of type \lsti{term -> term} were a
% \lsti{term} with a hole in it for another \lsti{term}.
The unfolding rule makes use of the Coq-Elpi primitives
\lsti{coq.safe-dest-app} in order to obtain the head term of a (possibly
nested) application, and \lsti{coq.env.indt} in order to access the global
environment of inductive definitions and query for information about them.
The \lsti{unfold_expert} predicate, then, has the role of selecting which
constructor to introduce for the inductive type. The kernel will successively
obtain the type of the selected constructor, and initiate the backchaining
phase on it.

In figure~\ref{fig:augmented}  each
inference rule is augmented with an additional premise involving an
\emph{expert predicate}, a certificate $\Xi$, and possibly continuations of
certificates ($\Xi'$, $\Xi_1$, $\Xi_2$) with extracted information from
These notions are taken from the general setting of \emph{foundational
  proof certificates}~\cite{chihani17jar}.
%
\todo{MM: Talk about randomized FPC?}
In our case here, an FPC is a collection of \lP clauses that
provide the remaining details not supplied in Figure~\ref{fig:augmented} and~\ref{fig:kernel}:
that is, the exact set of constructors for the  type of certificates as
well as the specification of the expert predicates listed \emph{ibidem}.
%
The top of Figure~\ref{fig:resources} displays two FPCs,
both of which can be used to describe proofs where we bound
the dimension of  a proof.
%
For example, the first FPC dictates that the query \mbox{\lsti{(check (qheight 5) B)}} is
provable in  the kernel using the clauses in Figures~\ref{fig:kernel}
and~\ref{fig:resources} if and only if the height of that proof is 5
or less.
%
Similarly, the second FPC can be used to bound the total number of instances of
unfoldings in a proof.
%
%In particular, the query \mbox{\lsti{(check (qsize 5 H) B)}}
% \begin{lstlisting}
% sigma H\ (check (qsize 5 H) B)
% \end{lstlisting}
%is provable if and only if the total number of unfoldings of that
%proof is 5 or less.

\begin{figure}[t]
\lstinputlisting[basicstyle=\ttfamily,language=lprolog,linerange={resources-end}]{code/fpcs.sig}
\lstinputlisting[basicstyle=\ttfamily,language=lprolog,linerange={resources-end}]{code/fpcs.mod}
\caption{Sample FPCs}
\label{fig:resources}
\end{figure}

\todo{We could move here from elab the paragraph ``It is also possible to pair two different proof certificates, defined
by two different FPC definitions,'' and/or reduce redundancy}
Further, if we view a particular FPC as a means of restricting proofs, it is
possible to build an FPC that \emph{restricts} proofs satisfying two FPCs
simultaneously.
%
% In particular, Figure~\ref{fig:pairing} defines an FPC based on the
% (infix) constructor \lsti{<c>}, which \emph{pairs} two terms of type
% \lsti{cert}.
%
The pairing experts for the certificate \lsti{Cert1 <c> Cert2} simply
request that the corresponding experts also succeed for both
\lsti{Cert1} and \lsti{Cert2} 
%
Thus, the query \lsti{check ((qheight 4) <c> (qsize 10)) B}
will succeed if there is a proof of \lsti{B} that has a height less
than or equal to 4 while also being of size less than or equal to 10.

% Various  additional examples and experiments using the pairing of FPCs
% can be found in~\cite{blanco17cade}. Using similar techniques, it is possible to define FPCs that target
% specific types for special treatment: for example, when generating
% integers, only (user-defined) small integers could be produced.


\subsection{A prolog-like tactic}

Thanks to the Coq-Elpi interface, in particular to the ``main''
proocedure \lsti{solve}, we can package the \lP's code for the checker
as a tactic that can be called as any other tactic in a Coq script.

\begin{lstlisting}
Elpi Tactic dprolog.
Elpi Accumulate lp:{{
  solve [str ''height'', int N] [goal _  Ev G _] _ :-
    coq.say "Goal:" {coq.term->string G},
    check (qheight N) (go G Term),
    Ev = Term,
    coq.say "Proof:" {coq.term->string Ev}.
...
(* Other clauses for different fpc omitted *)
}}.
\end{lstlisting}

The glue code between Coq-Elpi and the implementation of our calculus is
straightforward: the goal is given to us as a quadruple of a context, an
evar, a type and a list of extra information. In additon, the certificate is
supplied by the user when calling the tactic (it simply is an integer in this
case). We just need to call \lsti{check} on the goal's type, together with
the certificate, in order to obtain a reconstructed proof term. We avoid
calling the reconstruction directly on the evar because Coq-Elpi ensures that
evars manipulated by \lP are well-typed at all times, and we cannot guarantee
that since we work with partially reconstructed term; getting around this is
simply a matter of assigning the reconstructed term to the evar right
afterwards.
The following example shows how we can use the above tactic to do
FPC-driven logic programming in Coq and return a Coq proof-term:
\begin{lstlisting}
Inductive insert (x:nat) : list nat -> list nat -> Prop :=
| i_n : is_nat x -> insert x [] [x]
| i_s : forall y: nat, forall ys, x <= y -> insert x (y :: ys) (x :: y :: ys)
| i_c : forall y: nat, forall ys rs, y <= x -> insert x ys rs -> insert x (y :: ys) (x :: rs).
Lemma i1:  exists R, insert 2 [0;1] R.
eexists.
elpi  dprolog height 10.
Qed.
Print i1.
ex_intro (fun R : list nat => insert 2 [0; 1] R) [0; 1; 2]
  (i_c 2 0 [1] [1; 2] (i_c 2 1 [] [2] (i_n 2)) : exists R : list nat, insert 2 [0; 1] R
\end{lstlisting}

The \lsti{dprolog} tactic can be seen as a programmable version of
Coq's \lsti{auto}: it currently lacks the latter's capability of
solving goals by \lsti{reflexivity} as well as the modularity of using
\lsti{Hints} as introduction rules; on the bright side, it is not
restricted to depth-first search, since it follows the dictate of the
given FPC, for example iterative-deepening. Furthermore, the FPC can
provide a \emph{trace} that may be more customable that the one
offered by the \lsti{auto}'s hard-wired \lsti{Debug} facility.
%
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:

% LocalWords:  backchaining disjunct LJT dep qheight qsize orE someE
% LocalWords:  Elpi modularity Herbelin bc PTSC's dprolog CIC
