%\newpage
\section{Proof theory and proof certificates}
\label{sec:three}
  \newcommand{\XXi}{{\color{blue}{\Xi}}}

\todo{Anything here?}
% \begin{metanote}
%   (AM): Start with the logical reconstruction of \texttt{check}, to and then reverse engineer what we want/need to say about focusing and FPC. We \textbf{do} not talk about PBT yet, just logic programming (with proof terms). % In sec 4 we package \texttt{check} in a PBT query and explain how \texttt{check} works for term generation.
% \end{metanote}

\subsection{Focused proof systems with certificates}
\label{ssec:focused}
\todo{Change title? }
\todo{Use same names in Fig and 3: A for Atom, L for ListArgs etc.}



% The proof search approach to encoding Horn clause computation
% results in the structuring of proofs with repeated
% switching between a \emph{goal-reduction} phase and a
% \emph{backchaining} phase~\cite{miller91apal}.

%


%\subsection{Certificate checking with expert predicates}
%\subsection{Adding certificates and experts to a proof system}
%\label{ssec:fpc}

% \todo{Figure now uses Dale kernel-v2 --- needs further abstraction and explanation, see appendix}
\begin{footnotesize}
\begin{figure}[t]
  %
  \renewcommand{\Gamma}{}
    \newcommand{\bc}[5]{#1#2\vdash #3 : #4 \Downarrow #5}
\[
% \infer{\XXi\vdash t = t}
% {%\eqExpert{\XXi}
% }
% \qquad
% \infer{\XXi\vdash \true}
% {%\trueExpert{\XXi}
% }
% \qquad
% \infer{\XXi\vdash G_1\vee G_2}
% {\XXi\vdash G_i%\qquad \orExpert{\XXi}{\XXi'}{i}
% }
% \qquad
% \infer{\XXi\vdash \exists x. G}
% {\XXi\vdash G[t/x]%\qquad \someExpert{\XXi}{\XXi'}{t}
% }
% \qquad 
\infer{\XXi\Gamma\vdash A : s}
{(A,s) \in \mathcal{A}
}
% \qquad
% \infer{\XXi;\Gamma\vdash \lambda x:A.\,t : \forall x : A.\,  G}
% {\XXi;\Gamma, x : A\vdash t : G%\quad \andExpert{\XXi}{\XXi_1}{\XXi_2}
% }
\qquad
\infer{\XXi_1\Gamma\vdash k\ l : A}
      {\bc{\XXi_2}{\Gamma} l D A \quad k : D \in \Pscr_A\cup\Gamma\qquad \unfoldExpert{\XXi_1}{\XXi_2}} 
\]
\vskip -18pt
\[
  \infer{\bc {\XXi}{\Gamma} {[]} A A}{\trueExpert{\XXi}}{}\qquad
%  \infer{\bc{\forall x.\, D} A}{\bc{D[t/x]} A}{}\qquad
  \infer{\bc{\XXi}{\Gamma}{(t\ ::\ l)} {\forall x:B.\, D} A}
  {\XXi_1;\Gamma\vdash t : A\qquad \bc{\XXi_2}{\Gamma}l {B[t/x]} A\qquad\prodExpert{\XXi}{\XXi_1}{\XXi_2}}{}
\]
%\lstinputlisting[linerange={interp-end}]{code/dep-kernel.mod}
\caption{dep kernel.}
\label{fig:augmented}
\end{figure}
\begin{figure}
\lstinputlisting[linerange={sigs-end}]{code/dep-kernel.sig}
\lstinputlisting[linerange={check-end}]{code/dep-kernel-v2.mod}
\caption{Implementation of a  dependent kernel for Coq terms.}
\label{fig:kernel}
\end{figure}
\end{footnotesize}

Figure~\ref{fig:augmented} contains an implicitely focused proof system  for a small fragment of the Calculus of Inductive Constructions that corresponds to Horn logic programming over pure \lsti{Inductive} definitions.
%
It is a sequent calculus inspired by the calculus for
proof search in Pure Type Systems introduced in \cite{LengrandDM06}, based in
 turn on ideas stemming from focusing (in particular, the LJT calculus of
Herbelin \cite{Herbelin94}). Similarly to that calculus, we have a term
language that includes terms and lists of terms, and two distinct typing
judgements for the two categories. In our case, where proof search steps are
viewed as computation, the term judgement corresponds to a goal-reduction
step while the list judgement corresponds to a backchaining step. This coincides
with the idea behind the \emph{spine calculus}~\cite{Cervesato97tr}.

Since we are only dealing with types that correspond to Horn clauses, and
atomic types are inductively defined, we do not need a $\forall$ rule on the right, although the proof theory would gladly allow it. This means, at the term level, that we
don't need to have variables in our grammar of terms. Terms are always
applied to a (possibly empty) list of arguments, and as is usual in Pure Type
Systems we have the same grammar for terms and types:
%, as shown in figure~\ref{fig:terms}.
\todo{Let's think about that: we need A at least, but A is also a metavariable for atoms}
%\begin{figure}
\begin{align*}
t,u ::= & k\ l \;|\; \forall x : t.\, u \\
l ::= & [] \;|\; (t\ ::\ l)
\end{align*}
% \caption{Grammar of terms}
% \label{fig:terms}
% \end{figure}

\todo{This does not see quite right: D vs A, consider say even}
The proof system is parametrized by  external set of inductive definitions available in
the (implict) global context: we denote by $\Pscr_A$ the set of type declarations for the
constructors of type $A$, of the form $k : D$ where $D$ is of the form
$\forall x_1 : t_1.\, \dots \forall x_n : t_n.\, D\ u_1 \dots u_m$. We denote
by $\mathcal{A}$ the set of arity declaration for the inductive types.

%
%
%done Explain relations with other term assignments. Note that we do not use
% explicit substitutions (we are cut free)
% Explain the choice of not having certificates represent proof terms.
\todo{  Explain that the unfold clause handles
defined atoms and defined connectives (conjunction, disjunction, existential
and also equality): this may be less efficient and proof-theoretically induce
pointless phase changes, but it is arguably very elegant e simplifies proof
terms that arand inherited by Coq }

Figure~\ref{fig:kernel} contains the Elpi implementation of the
inference rules in Figure~\ref{fig:augmented}: $\XXi\vdash t \colon A$ corresponds to
\lsti{check $\XXi$ (go A T)} and $\XXi\vdash l \colon D \Downarrow A$ corresponds to
\lsti{check $\XXi$ (bc D A L)}.

% here the infix
% turnstile $\vdash$ symbol is replaced by the \lsti{check} predicate, and every rule
% is directly mapped to a clause in the code. The argument of the clause is
% either given by a \lsti{go} or a \lsti{bc} constructor, signaling whether the clause
% is a goal-reduction or a backchaining one.

Coq terms are accessed through the Coq-Elpi API, and their representation
in \lP takes advantage of the native \lP constructs such as lists and binders.
The following is a list of some of signatures of the notions that we use:
\begin{lstlisting}
  kind term   type.   % reification of Coq terms
  kind gref    type     % reification of refernces to global terms
  type global gref -> term.
  type indt   inductive -> gref.  % reification of inductive types
  type indc   constructor -> gref. % reification of their constructors 
  type app    list term -> term.  % reification of nary application
  type prod   name -> term -> (term -> term) -> term.
\end{lstlisting}

Note that \lsti{prod} encodes dependent products taking a name for pretty printing,  a term and a \lP function from
terms to terms, namely $\forall x : A, B$ is encoded by \lsti{prod ``x'' A (x\ B x)}; when, in the implementation of the product-left rule, we
apply $B$ to the variable \lsti{Tm},
we get a term that can be used to continue backchaining. This
application is obtained via meta-level substitution, in the style of HOAS. In this sense, our calculus uses \emph{implict} substitutions, rather than explicit ones as in LJT and PTSC's tradition, the more since proof search in our application is cut-free. 
% Syntax: one can think as if something of type \lsti{term -> term} were a
% \lsti{term} with a hole in it for another \lsti{term}.
\todo{Now safe-dest-app is hiiden inside \texttt{definition}. Put the rest of the code in the appendix?}
The unfolding rule makes use of the Coq-Elpi primitives
\lsti{coq.safe-dest-app} in order to obtain the head term of a (possibly
nested) application, and \lsti{coq.env.indt} in order to access the global
environment of inductive definitions and query for information about them.



\smallskip
In figure~\ref{fig:augmented}  each
inference rule is augmented with an additional premise involving an
\emph{expert predicate}, a certificate $\Xi$, and possibly continuations of
certificates ($\Xi'$, $\Xi_1$, $\Xi_2$) with extracted information from
These notions are taken from the general setting of \emph{foundational
  proof certificates}~\cite{chihani17jar}.
%
In our case here, an FPC is a collection of \lP clauses that
provide the remaining details not supplied in Figure~\ref{fig:kernel}:
that is, the exact set of constructors for the  type of certificates as
well as the specification of the expert predicates listed \emph{ibidem}.
%
The top of Figure~\ref{fig:resources} displays two such FPCs,
both of which can be used to describe proofs where we bound
the dimension of  a proof.
%
For example, the first FPC dictates that the query \mbox{\lsti{(check (qheight 5) B)}} is
provable in  the kernel using the clauses in Figures~\ref{fig:kernel}
and~\ref{fig:resources} if and only if the height of that proof is 5
or less.
%
Similarly, the second FPC can be used to bound the total number of instances of
unfoldings in a proof.
%
%In particular, the query \mbox{\lsti{(check (qsize 5 H) B)}}
% \begin{lstlisting}
% sigma H\ (check (qsize 5 H) B)
% \end{lstlisting}
%is provable if and only if the total number of unfoldings of that
%proof is 5 or less.

\begin{figure}[t]
\lstinputlisting[linerange={resources-end}]{code/fpcs.sig}
\lstinputlisting[linerange={resources-end}]{code/fpcs.mod}
\caption{Sample FPCs}
\label{fig:resources}
\end{figure}

\todo{We could move here from elab the paragraph ``It is also possible to pair two different proof certificates, defined
by two different FPC definitions,'' and/or reduce redundancy}
Further, if we view a particular FPC as a means of restricting proofs, it is
possible to build an FPC that \emph{restricts} proofs satisfying two FPCs
simultaneously.
%
% In particular, Figure~\ref{fig:pairing} defines an FPC based on the
% (infix) constructor \lsti{<c>}, which \emph{pairs} two terms of type
% \lsti{cert}.
%
The pairing experts for the certificate \lsti{Cert1 <c> Cert2} simply
request that the corresponding experts also succeed for both
\lsti{Cert1} and \lsti{Cert2} 
%
Thus, the query \lsti{check ((qheight 4) <c> (qsize 10)) B}
will succeed if there is a proof of \lsti{B} that has a height less
than or equal to 4 while also being of size less than or equal to 10.

% Various  additional examples and experiments using the pairing of FPCs
% can be found in~\cite{blanco17cade}. Using similar techniques, it is possible to define FPCs that target
% specific types for special treatment: for example, when generating
% integers, only (user-defined) small integers could be produced.

\begin{metanote}
  AM stops here
\end{metanote}
\subsection{A prolog-like tactic}

\begin{metanote}
  Let's dump it here and see how it looks
\end{metanote}

\begin{lstlisting}
Elpi Accumulate lp:{{
  solve [str ''height'', int N] [goal _Ctx Ev G _Who] _OutGoals :-
    coq.say "Goal:" {coq.term->string G},
    check (qheight N) (go G Term),
    Ev = Term,
    coq.say "Proof:" {coq.term->string Ev}.
...
}}.
\end{lstlisting}
The following example show how we can do FPC-driven logic programming in Coq and return
a Coq term
\begin{lstlisting}
Inductive insert (x:nat) : list nat -> list nat -> Prop :=
| i_n : is_nat x -> insert x [] [x]
| i_s : forall y: nat, forall ys, x <= y -> insert x (y :: ys) (x :: y :: ys)
| i_c : forall y: nat, forall ys rs, y <= x -> insert x ys rs -> insert x (y :: ys) (x :: rs).
Lemma i1:  exists R, insert 2 [0;1] R.
eexists.
elpi  dprolog 10.
Qed.
Print i1.
ex_intro (fun R : list nat => insert 2 [0; 1] R) [0; 1; 2]
  (i_c 2 0 [1] [1; 2] (i_c 2 1 [] [2] (i_n 2)) : exists R : list nat, insert 2 [0; 1] R
\end{lstlisting}

The \lsti{dprolog} tactic can be seen as a programmable version of
Coq's \lsti{auto}: it currently lacks the latter capability of solving
goals by \lsti{reflexivity} and the modularity capability of using
\lsti{Hints}, but it is not compelled to use depth-first search, since it follows the dictate of the given FPC. For example the FPC can provide a \emph{trace}, which may be more customable that the one offered by the \lsti{Debug} option.
%
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:

% LocalWords:  backchaining disjunct LJT dep qheight qsize orE someE
% LocalWords:  Elpi modularity
