\section{Revisiting property-based testing for Coq}

We have presented in a previous paper~\cite{blanco19ppdp} a
proof-theoretical reconstruction of property-based testing~\cite{fink97sen} (PBT) of
relational specifications adopting techniques from foundational proof
certificates to account for several features of this testing
paradigm: from various \emph{generation} strategies, to
\emph{shrinking} and fault localization.

Given the connection that \textsf{coq-elpi} offers between logic
programming and the internals of Coq, it is natural to extend the
FPC-driven logic programming interpreter of the previous section to
perform PBT over \lstinline{Inductive} types.

While nothing prevents us from porting all the features we discussed
in~\cite{blanco19ppdp}, for the sake of this paper we will implement
only FPC corresponding to different flavors \emph{exhaustive}
generation, as adopted, e.g., in SmallCheck~\cite{smallcheck} and
$\alpha$Check~\cite{cheney_momigliano_2017}, and their combination. Note however that it would take additional two lines of code to the \lsti{unfold} expert to implement a form of random data generation in the sense of randomized backtracking~\cite{pltredexconstraintlogic}.

Of course, Coq already features \textsf{QuickChick}~\cite{QChick}
(\url{https://softwarefoundations.cis.upenn.edu/qc-current}),  a
sophisticated and well-supported PBT tool, based on a different
perspective: being a clone of Haskell's QuickCheck, it emphasizes
testing executable (read \emph{decidable)} specifications with random
generators. While current research~\cite{LampropoulosPP18} aims to
increase automation, it is fair to say that testing with ~\cite{QChick}, in particular of
relational specifications, is still very labor intensive. We do not
intend to compete with \textsf{QuickChick}, but we shall see that we
can test immediately \lsti{Inductive} definitions that
corresponds to pure Horn programs, without any decidability proof nor
commitment to the random generation strategy, and related
configuration of generators and shrinkers.

\todo{Drop the subsections?}
\subsection{PBT as proof reconstruction}
\label{ssec:pbt-lp}

% \begin{metanote}
%   I'm just saying the bare minimum -am
% \end{metanote}

If we view  a property as a many-sorted logical formula:
\[
  \forall x_1 \colon \tau_1\dots x_n \colon \tau_m, A_1\land\dots\land A_m \supset B\tag{*}\label{eq:prop}
  \]
%\(\forall x [(\tau(x)\wedge P(x)) \supset Q(x)]\) where $\tau$ is a
%typing predicate and
  where the $A_i$ and $B$ are  predicates defined
using Horn clause specifications, providing a counter-example
consists of  negating the property, and \emph{searching} for a proof of
\[
  \exists x_1 \colon \tau_1\dots x_n \colon \tau_n [A_1\land\dots\land A_m\wedge \neg B]
  \tag{**}\label{eq:full}
\]
%\(\exists x [(\tau(x)\land P(x)) \land \neg Q(x)]\).

In our Coq current setting $A_i$ and $B$ will be \lsti{Inductive}
propositions, while the $\tau_j$ are honest-to-goodness datatypes. We
will treat the two quite differently, in so far as elements of those
types will be \emph{generated}, while predicates will only be
\emph{checked}. This distinction is reminiscent of bi-directional
type-checking and  plays also a part in interpreting the
negation sign.
%
When using a focused proof system for logic extended with fixed
points~\cite{baelde12tocl}, negation corresponds to the usual
intutionistic interpretation, which is  what Coq supports.  % proofs of formulas such as
% \[
%   \exists x [(\tau(x)\wedge P(x)) \wedge \neg Q(x)]
%   \tag{*}\label{eq:full}
% \]
% are a single \emph{bipole}: that is, when reading a proof bottom up, a
% positive phase is followed on all its premises by a single negative
% phase that completes the proof.%
% In particular, the positive phase corresponds to the \emph{generation} phase
% and the negative phase corresponds to the \emph{testing} phase.
% %

% Instead of giving a full focused proof system of a logic including
% fixed points (since, as we will argue, that proof system will not, in
% fact, be needed to account for PBT ), we offer the following analogy.
% %
% Suppose that we are given a finite search tree and we are asked to
% prove that there is a secret located in one of the nodes of that
% tree.
% %
% A proof that we have found that secret can be taken to be a
% description of the path to that node from the root of the tree: that
% path can be seen as the proof certificate for that claim.
% %
% On the other hand, a proof that no node contains the secret is a
% rather different thing: here, one expects to use a blind and
% exhaustive search (via, say, depth-first or breath-first search) and
% that the result of that search never discovers the secret.
% %
% A proof of this fact requires \emph{no} external information: instead it
% requires a lot of computation involved with exploring the tree until
% exhaustion.
% %
% This follows the familiar pattern where the positive (generate) phase
% requires external information while the negative (testing) phase
% requires none.
%
However, for the sake of PBT and as we argued in~\cite{blanco19ppdp}  , we can identify a proof certificate for
$(\ref{eq:full})$ simply with a proof certificate for
$
 \exists x_1 \colon \tau_1\dots x_n \colon \tau_n [A_1\land\dots\land A_m].
$
and we can resort to negation-as-failure to check that the conclusion does not hold without caring for any evidence for the latter. This also means that we do not produce a Coq proof terms for the refutation of our property --- and neither does \textsf{QuickChick}, which runs at the OCaml level --- although we can return the witness for the existential.
% Such a certificate would contain the witness (closed) term $t$ for the
% existential quantifier and sufficient information to confirm that
% $P(t)$ can be proved (a proof of a typing judgment such as $\tau(t)$
% is usually trivial).
% Since a proof certificate for the existence-of-a-counterexample formula
% $(\ref{eq:full})$ can be taken as a proof certificate of
% $(\ref{eq:short})$, then we only need to consider proof certificates
% for Horn clause programs.
%
%Since the logic of Horn clauses is rather simple,
% We illustrate next what such proofs and proof certificates look like
% for the rather simple logic of Horn clauses.



\subsection{An Elpi tactic for PBT}
\label{ssec:dep_pbt}
We will invoke the tactic in a proof environment where the overall
goal is % one is trying to prove is the
the property that the system-under-test (SUT) should meet. This means that,
after \lsti|intro| has been used to introduce the relevant hypotheses,
% we have a proof
% context % consisting of typing information for a bunch of variables
% where eigenvariables play different roles: dependent variables, i.e., the ones appearing also in the types of other variables, are
% subject to \emph{generation}, while non-dependent ones tag assumptions that
% are only to tested.  % Obviously, the
% % two are the result of and \lsti|intro| on either a dependent or a
% % non-dependent product
% % ; in our setting this division acquires an additional reading beyond
% % what the property looks like: since specifications are encoded as pure
% % Horn clauses where no dependency is involved, non-dependent variables
% % simply tag the specification clauses, whereas dependent variables
% % represent the typing information for the data variables that the
% % specifications use.

% Within the tactic
the user therefore
specifies which variables of the environment should be used for
generating data and which for executing the specification. In addition
to this, the user should specify all the certificate information that
will guide the data generation phase. % The goal of the current
% environment will directly provide the property for which we want to
% find a counterexample.
% Say we want to generate a counterexample for
% some specification represented by \lsti|H1| and \lsti|H2| in the
% environment, and we want to generate the data for some variables
% \lsti|v1| and \lsti|v2| with a height bounded to 10: the final call to
% the tactic will then look like:
% \begin{lstlisting}
%   elpi pbt 10 (H1 /\ H2) (v1) (v2)
% \end{lstlisting}
Concretely, for the property~$(\ref{eq:prop})$
% \[
% \forall x_1 \colon \tau_1\dots x_n \colon \tau_m, A_1\land\dots\land A_m \supset B
% \]
and the specification of a FPC, the call to the tactic will be:
\begin{lstlisting}
elpi dep_pbt <fpc> $(A_1\land\dots\land A_m) \ (x_1) \dots (x_n)$.
\end{lstlisting}
The tactic calls \lsti{check} with the given FPC on the dependent
variables and will simply call a vanilla meta-interpreter (see
Fig.~\ref{fig:interp} in the appendix) to test the hypotheses and the negation of the
goal:
\begin{lstlisting}
interp $(A_1\land\dots\land A_m$), not (interp $B$)}
\end{lstlisting}
where \lsti{not} is \lP's negation-as-failure operator.

To exemplify, let us add to the previous specification of list
insertion a definition of ordered list:
\begin{lstlisting}
Inductive ordered : list nat -> Prop :=
| o_n : ordered []
| o_s : forall x : nat, ordered [x]
| o_c : forall (x y : nat), forall xs,  ordered xs  -> x <= y -> ordered (x:: y :: xs).     
\end{lstlisting}
%
A property we may wish to check before embarking in a formal proof is
whether insertion preserves ordered-ness:
\begin{lstlisting}
Conjecture ins_ord:  forall (x : nat) xs rs, ordered xs -> insert x xs rs -> ordered rs.
intros x xs rs Ho Hi.
elpi dep_pbt height 5 (Ho /\ Hi ) (x) (xs).
Abort.
\end{lstlisting}
In this query the tactic tests the hypotheses \lsti{Ho} and \lsti{Hi}
against data \lsti{x,xs} generated exhaustively up to height
at most $5$  out of the library \lsti{Inductive} definitions of
\lsti{nat} and \lsti{list}. We do not generate values for \lsti{rs},
since by mode information we know that it will be computed.
%
Since we did slip in an error (where?), our tactic reports a
counter-example, namely \verb|Proof Term: [0, [0; 1; 0]]|, and
we therefore abort the proof.


In order to generate the PBT query, some
pre-processing is needed. In particular, we need to turn the data
eigenvariables into \lP logic variables when they appear inside a
specification, and to generate queries %to \lsti|check|
for each of
these logic variables in association with the type of the data
variable it corresponds to.  In order to realize this pre-processing step,
we leverage extensively \lP's higher order programming features. The
substitutions are handled with the technique of \lsti|copy| clauses~\cite{miller91jlc}.

%\todo{Matteo: we're doing some preprocessing for interp as well, right?}
Note that testing the above conjecture with \textsf{QuickChick} would
have required much more setup: if we wished to proceed relationally as
above, we would have had to provide a proof of decidability of the
relevant notions. Were we to use functions, then we would have to
implement a generator and shrinker for ordered lists, since automatic
derivation of the former does not (yet) work for this kind of specification.

%\todo{the \texttt{aexp} example}

For a more significant case study, let us turn to the semantics of
programming languages, where PBT has been used extensively and
successfully~\cite{Klein12}. Here we will consider a far simple example,
a typed arithmetic language featuring numerals with predecessor
and Booleans with if-then-else and test-for-zero, which comes from the \emph{Software Foundations} book series
(\url{https://softwarefoundations.cis.upenn.edu/plf-current/Types.html}):
\begin{lstlisting}
Inductive tm : Type :=
 | ttrue : tm | tfalse : tm | tif : tm -> tm -> tm -> tm | tzero : tm | tsucc : tm -> tm
 | tpred : tm -> tm | tiszero : tm -> tm.
 Inductive typ : Type := |TBool : typ |TNat : typ.
\end{lstlisting}
The completely standard static and small step dynamic semantics are
reported in appendix~\ref{ssec:types}.

While it is obvious that \emph{subject expansion} fails for this calculus, it is gratifying to have it confirmed by our tactic, with counterexample \lsti{e = tif ttrue tzero ttrue}:
\begin{lstlisting}
Conjecture subexp:  forall e e' t, step e e' -> has_type e' t -> has_type e t. 
 intros e e' t HS HT.
 elpi dep_pbt height 2 (HS /\ HT) (e).
\end{lstlisting}

Another way to asses the fault detection capability of a PBT setup is via \emph{mutation analysis}~\cite{CavadaCM20}, whereby localized bugs are voluntarily inserted, with the view that they should caught (``killed'') by a ``good enough'' testing suite. Following on  an exercise if the aforementioned chapter of SF, we modify the typing relation by adding the following (nonsensical)  clause:

\begin{lstlisting}
Module M1.
Inductive has_type : tm -> typ -> Prop :=
$\dots$
  | T_SuccBool : forall t, has_type t TBool -> has_type (tsucc t) TBool.
end M1.
\end{lstlisting}
Some of the required properties for our SUT now fails: not
only type uniqueness, but also progress with counterexample \lsti{e = tsucc ttrue}:
\begin{lstlisting}
Definition progress (e :tm) (Has_type : tm -> typ -> Prop) (Step : tm -> tm -> Prop):= 
     forall t, Has_type e t -> notstuck e Step.
Conjecture progress_m1:  forall e, progress e M1.has_type step.
 unfold progress.
 intros e t Ht.    
 elpi dep_pbt 2 height (Ht) (e) .  
\end{lstlisting}

Whereas examples based on the \emph{Types} chapter of SF are
admittedly quite simple-minded, they have been used as a benchmark for
evaluating \textsf{QuickChick}'s automation of generators under
invariants~\cite{LampropoulosPP18}, and to be amenable to the tool,
the specification had to be massaged non-insignificantly.
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "main"
%%% End:

% LocalWords:  Bedwyr bipole elpi QuickCheck OCaml TODO coq xs nat
% LocalWords:  tif ttrue tzero tsucc SUT
