
\documentclass{beamer}
\mode<presentation>
%\usepackage{tikz}
%\usetheme{Frankfurt}
\usecolortheme{seahorse}
%\usepackage{xcolor}
\usepackage{graphics}
\usepackage{amsthm,verbatim,booktabs}
\usepackage{epsfig}
%\input{nominal}
\usepackage{boxedminipage}     % for surrounding boxes in figures
\usepackage{proof}
\usepackage{amsmath} 
\usepackage{xspace}
\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\NF}{\emph{NAF}\xspace}
\newcommand{\trueExpert }[1]{{\true_e}(#1)}
\newcommand{\eqExpert }[1]{{=_e}(#1)}
\newcommand{\unfoldExpert}[2]{{\hbox{\sl unfold}_e}(#1,#2)}
\newcommand{\andExpert}[3]{{\wedge_e}(#1,#2,#3)}
\newcommand{\andExpertLJF}[6]{{\wedge_e}(#1,#2,#3,#4,#5,#6)}
\newcommand{\orExpert  }[3]{{\vee_e}(#1,#2,#3)}
\newcommand{\someExpert}[3]{\exists_e(#1,#2,#3)}
\newcommand{\initExpert}[2]{\hbox{\sl init}_e(#1,#2)}
\newcommand{\lP}{$\lambda$Prolog\xspace}
\newcommand{\true}{tt}
  \newcommand{\XXi}{{\color{blue}{\Xi}}}
\newcommand{\ok}{\checkmark}
  \def\lam{\lambda}
\def\Lam{\Lambda}
\def\arrow{\rightarrow}
\def\oftp{\mathord{:}}
\def\hastype{\mathrel{:}}
\def\bnfas{\mathrel{::=}}
\def\bnfalt{\mid}

\newenvironment{myslide}[1]
    {\begin{frame}\frametitle{#1}}
    {\end{frame}}

\newcommand{\new}[1][]{\reflectbox{\sf{#1{}N}}}
% \newcommand{\aprolog}{{\ensuremath \alpha}{Pro\-log}\xspace}
% \newcommand{\acheck}{{\ensuremath \alpha}{Check}\xspace}
\newcommand{\lprolog}{{\ensuremath \lam}{Pro\-log}\xspace}
\newcommand{\plus}{\hbox{\sl app}}

%\newtheorem{theorem}{Theorem}
%\newtheorem{lemma}{Lemma}
%\newtheorem{conj}{Conjecture}
\usepackage{listings}
\input{listing-macros}
\lstset{language=lprolog}
\lstset{language=abella}
\begin{document}



\title{
Property-Based Testing to Infinity and beyond!}


\author{Alberto Momigliano\\ joint work with {Roberto Blanco} and Dale Miller}
\institute{LFMTP 2020}
\date{June 29, 2020}

\begin{frame}
\titlepage
\end{frame}
 %---------------------------------------------------------
\begin{frame}
  \frametitle{Take away from the talk}
  \begin{itemize}
  \item Standard PBT  is just search in a focused sequent calculus akin to uniform proofs
  \item PBT's data generation strategies (exhaustive, random) and
    other features (shrinking, bug provenance) can be programmed as
    instantiations of the Foundational Proof Certificates paradigm.
  \item Co-PBT requires a stronger logic with fixed points, preferably linear.
  \end{itemize}
    % \pause
    
   Now you can go (back) to sleep.

\end{frame}


%\section{Part 1: Motivation}
%----------------------------------------------------------------------------
%\begin{frame}[fragile]\frametitle{Off the record}
%-------------------------------------------------
 %  \begin{itemize}
%   \item After almost 20 years of formal verification with Twelf,
%     Isabelle/HOL, Coq, Abella, I'm a bit worn out
%   \item I still find it  a very demanding, often frustrating, day
%     job.
% % \pause
%   \item Especially when the theorem I'm trying to prove is, ehm,
%     wrong. I mean, \emph{almost right}:
% % \pause
%     \begin{itemize}
%     \item statement is too strong/weak
%     \item there are minor mistakes in the spec I'm reasoning about
%     \end{itemize}
%   \item A failed proof attempt not the best way to debug those kind of
%     mistakes
%   \item That's why I'm inclined to give \emph{testing} a try (and I'm
%     in good company!)  % \pause
% \item Not any testing: \textbf{property-based testing}
%   \end{itemize}

% \end{frame}
%----------------------------------------------------------------------------
\begin{frame}[fragile]\frametitle{Property-based Testing}
  \begin{itemize}
  \item A light-weight validation approach to software correctness
    % \begin{enumerate}
    % \item automatic generation of test data, against
    % \item executable program specifications.
    % \end{enumerate}
    % % \pause
  \item The programmer specifies executable properties  the code should satisfy
  \item PBT tries to refute them  by trying a  large number of
    automatically generated cases.
    % \pause
     \item Brought together in \emph{QuickCheck} (Claessen \& Hughes '00) for Haskell \dots
       
     \item \dots and now available for pretty much every PL (and
       commercialized for Erlang).
  \end{itemize}


\end{frame}

% %----------------------------------------------------------------------------
\begin{frame}[fragile]
  \frametitle{FsCheck example: 1/2}
  Does insertion in an ordered list preserve order?
  \begin{small}
    \begin{lstlisting}
let rec ordered = function                      
 |[] -> true
 |[x] -> true
 |x::y::ys -> x <= y && ordered ys  

let rec insert x  = function 
 |[] -> [x]
 |c::cs -> 
   if x <= c then x::c::cs else c::insert x cs
 
let prop_insert (x:int,  xs) =  
  ordered xs ==> ordered (insert  x xs)

do Check.Quick prop_insert
    \end{lstlisting}
  \end{small}

% \pause
  \begin{small}
    \begin{lstlisting}
Falsifiable, after 16 tests ... (0, [0; 0; -1])
    \end{lstlisting}
  \end{small}
\end{frame}

% %----------------------------------------------------------------------------
\begin{frame}[fragile]
  \frametitle{FsCheck example: 2/2}
Well, it would, had I the correct encoding of \texttt{ordered}
  \begin{small}
    \begin{lstlisting}
let rec ordered = function                      
   |[] -> true
   |[x] -> true
   |x::y::ys -> x <= y && ordered (y :: ys) 
                    // was ordered ys  
 
// rest stays the same

do Check.Quick prop_insert
 \end{lstlisting}
  \end{small}
  % \pause
    \begin{small}
    \begin{lstlisting}
OK. Arguments exhausted after 37 tests.
    \end{lstlisting}
  \end{small}
  Since this is a conditional property with a  sparse pre-condition, it
  warns me of bad coverage, but this is another story\dots
\end{frame}

%\begin{frame}[fragile]\frametitle{QuickCheck's Hello World!\hfill
%     (FsCheck, actually)}
%   \begin{small}
% \begin{verbatim}
% let rec rev ls =  
%     match ls with 
%     | [] -> [] 
%     | x :: xs -> append (rev xs, [x]) 

% // a true spec
% let prop_revRevIsOrig (xs:int list) =  rev (rev xs) = xs;;

% do Check.Quick prop_revRevIsOrig ;;
% >> Ok, passed 100 tests.

% // a false spec
% let prop_revIsOrig (xs:int list) = rev xs = xs

% do Check.Quick prop_revIsOrig ;;

% >>  Falsifiable, after 3 tests (5 shrinks) (StdGen (518275965,...)):
% [1; 0]
%   \end{verbatim}
%   \end{small}
%   % \begin{itemize}
%   % \item 
%   % \end{itemize}

% \end{frame}

\begin{frame}
  \frametitle{PBT and theorem proving}
  \begin{itemize}
  \item One of PBT's success stories is the integration with proof assistants
  % \pause
  \item Isabelle/HOL broke the ice adopting
    \emph{random} testing some 15 years ago %and many     followed suit:
    \begin{itemize}
    \item  a la QC: Agda ['04], PVS ['06], Coq
    with QuickChick ['15]
  \item exhaustive/smart generators (Isabelle/HOL ['12])
  \item model finders (Nitpick, again in Isabelle/HOL ['11])
  \end{itemize}
% \item In fact, Pierce and co.'s \emph{Software Foundations} has a volume dedicated to PBT.
%   % \pause
\item But, wait! Isn't testing the very thing theorem proving wants to replace?
  % \pause 
\item Oh, no: test a conjecture before attempting to prove it and/or
  test a subgoal (a lemma) inside a proof
\item The beauty (wrt general testing) is: you don't
  have to invent the specs, they're exactly what you want to prove
  anyway.
  \end{itemize}
\end{frame}
% ---------
\begin{frame}\frametitle{My pet interest: \emph{mechanized
    meta-theory verification}}
\begin{itemize}
\item Consider the verification of the meta-theory of programming languages and related calculi with a proof assistant.
\item  Type soundness, proofs by logical relations, non-interference\dots
\item Proofs are mostly standard, but
  \begin{itemize}
  \item lots of work, mostly wasted in the
  \red{design} phase
\item only worthwhile
  if \red{we already ``know'' the system is correct}
\end{itemize}

% \pause
\item PBT to the rescue:
\begin{itemize}
  \item produces helpful counterexamples for incorrect systems
%  \item unhelpfully diverges for correct systems
  \item little expertise required, fully (well, sort of) automatic
\end{itemize}
  \end{itemize}
\end{frame}

% ----------------------------------------------------------------------------
\begin{frame}
  \frametitle{What about infinite computations?}
  \begin{itemize}
  \item I don't have to argue (but I will) about the relevance of
    coinduction/corecursion in PL theory:
    \begin{itemize}
    \item divergence/co-evaluation
    \item recursive (sub)typing
    \item program equivalence
    \item pretty much the whole meta-theory of process calculi
    \item \dots [add your own]
    \end{itemize}
    % \pause
  \item Lots of work on the \textbf{proving} side in most proof assistants
    \begin{itemize}
    \item guarded recursion, greatest fixed points, co-patterns, co-datatypes \dots
    \end{itemize}
    % \pause
  \item Much less on the \textbf{testing} side:
    \begin{itemize}
    \item a little with Haskell's QC (but basically using the \emph{take} lemma)
    \item a little with \emph{Nitpick}, possibly with (co)datatypes, I have to check
    \item seems hard for Coq's QuickChick.
    \end{itemize}

  \end{itemize}
\end{frame}


% ----------------------
\begin{frame}
  \frametitle{Motivating examples: co-evaluation}
  \newcommand{\step}{\Downarrow}
  \begin{itemize}
  \item Consider the CBV big step semantics of the lambda calculus,
    but \red{coinductively} [Leroy \& Grall '07]:
    \begin{small}
  \[
  \begin{array}{c}
    % \infer=[\texttt{E-V}]{V \step V}{\mathrm{value\ } V}
        \infer=[\texttt{E-L}]{\lambda x.\, M \step \lambda x.\, M}{}
  \qquad
    \infer=[\texttt{E-A}]{M_1\cdot M_2 \step V}{M_1 \step \lambda x.\ M\quad M_2\step V_2\quad  M\{V_2/x\}\step V}
    \end{array}
  \]
\end{small}
\item Is this deterministic? Is it type preserving?
  % \pause
  
\item No, on both counts:
  \begin{itemize}
  \item a divergent term such as $\Omega$ co-evaluates to anything.
  \item a variant of the Y-combinator falsifies preservation [Filinski].
  \end{itemize}
  
\item In fact, one can argue that this notion of co-evaluation makes
  little sense [Ancona et al. '15] and wouldn't it be nice if your PBT
  tool would help you with that?
\end{itemize}
\end{frame}
% ----------------------
\begin{frame}
  \frametitle{Motivating examples: separating equivalences}
  \begin{itemize}
  \item Consider  \textbf{PCF} with lazy lists and
    observational equivalence via \red{bisimilarity}
    [Pitts '98]
  \item There are several notions of equivalence, e.g., depending if you observe
    convergence at any type (\red{applicative}) or at base type
    (\red{ground}). Can we separate them?
    \begin{itemize}
    \item $\lambda x.\, \perp$ is \red{ground} but not \red{applicative} bisimilar to $\perp$
    \item Same for the eta and surjective pairing laws. 
      \end{itemize}
 
  \item Similar results in the $\pi$ calculus:
    \begin{itemize}
    \item Ground bisimilarity not a congruence
      \item  Early and late bisimilarity not preserved by inputs
    \end{itemize}
  \end{itemize}
\end{frame}
% ----------------------
\begin{frame}
  \frametitle{\textbf{Non}-examples}
  \begin{itemize}
  \item What about \red{streams}? All those nice Haskell-like equations, as in Louise Dennis' thesis?
    % \pause
  \item Thanks, but no thanks
    % \pause
  \item Here we concentrate on \red{infinite behavior} (e.g.,
    is a finite program diverging) rather than \red{infinite objects}
    (e.g., is $2$ the last element of the infinite stream $1::1::1\dots$?).
  \item In a logical view of coPBT, we would need to
    \textbf{construct} such infinite terms as cex, and the literature is not satisfactory:
    % \pause
    \begin{itemize}
    \item coinductive LP works with rational terms: problematic and hopelessly incomplete
% \pause
    \item ``Coinduction in uniform'' uses a fixed point term
      constructor: does not sit well within a logical framework
      (adequacy of encodings, canonical forms etc.).
    \end{itemize}
  \end{itemize}
\end{frame}

% \begin{frame}[fragile]\frametitle{Not so fast/quick\dots}

%     \begin{small}
% \begin{verbatim}
% ordered xs ==> ordered (insert x xs)
% \end{verbatim}
%     \end{small}

  
% \end{frame}
%----------------------------------------------------------------------------







%---------------------------\-------------
\begin{myslide}{PBT: from FP to LP}
  \begin{itemize}
%  \item Lots of activities, mainly in the FP community --- lack of logical common thread
  \item PBT  was born and raised functional, but is rediscovering logic programming:
    \begin{itemize}
    \item mode analysis in Isabelle/HOL and QuickChick's automatic derivation of 
        generators
    \item (Randomized) backchaining in PLT-Redex
    \item Narrowing in LazySmallCheck \dots

    \end{itemize}
  % \pause
  \item What the last 30 years have taught us is that if we take a
    \red{proof-theoretic} view of LP, good things start to happen
  \item In particular: \red{focusing} and a \red{treatment of
      bindings}
  % \item While standard PBT can be accounted for by basically
  %   \red{focused proofs} on a bit more than Horn logic, for coPBT we
  %   need to resort to stronger (fixed points) logic, but still keep a LP vibe.
 
  \end{itemize}
\end{myslide}
% ----------------------
\begin{frame}
  \frametitle{PBT: the logical view}
  \begin{itemize}
  \item Specifications and code (think \texttt{ordered} or more interestingly the operational semantics of a PL) are logical theories 
  % \pause
  \item Trying to refute a property of the form
    \[\forall x\colon\tau\ [P(x) \supset Q(x)]\] means searching for a (focused) proof of
    \[\exists x [(\tau(x)\land P(x)) \land \neg Q(x)]\] yielding a
    a $t$ of type $\tau$ s.t.\ $P(t)$ holds and $Q(t)$ does not % \pause
  \item The generate-and-test approach of PBT can be seen in terms of
    focused sequent calculus proof where the \red{positive} phase
    corresponds to generation and a single \red{negative} one to
    testing.
  \item Intuition: generating is hard (lots of backtracking), testing is easy (deterministic computation).
  \end{itemize}
\end{frame}

% ----------------------
\begin{myslide}{Going deeper: PBT via FPC}
  \begin{itemize}
  \item A flexible  way to look at those proofs is as a
    \red{proof reconstruction} problem in the \red{Foundational Proof
      Certificate} framework [Chihani, Miller \& Renaud 2017]
\item FPC proposed as a means of defining proof
structures used in a range of different theorem provers
% \pause
\item Think of a focused sequent calculus
  augmented with predicates (\textbf{clerks} for the negative phase and \textbf{experts}
  for the positive one) that produce and process information to drive
  the checking/reconstruction of a proof.
% \pause
\item For PBT, use of FPC as a way to
describe \red{generation strategies} and as a way to combine them.
  \end{itemize}
  
\end{myslide}
%----------------
\newcommand{\andd}{\wedge}
\newcommand{\impp}{\supset}

\newcommand{\instan}[1]{\hbox{\sl grnd}~(#1)}
\newcommand{\Pscr}{{\mathcal P}}
%
% -----------------------------------------------
\begin{frame}
  \frametitle{Proof system for Horn logic with certificates and experts}

\[
\infer{\XXi\vdash G_1\wedge G_2}
      {\XXi_1\vdash G_1\qquad \XXi_2\vdash G_2\qquad \andExpert{\XXi}{\XXi_1}{\XXi_2}}
\qquad
\infer{\XXi\vdash \true}
      {\trueExpert{\XXi}}
\]
\vskip -6pt
\[
\infer{\XXi\vdash G_1\vee G_2}
      {\XXi'\vdash G_i\qquad \orExpert{\XXi}{\XXi'}{i}}
\qquad
\infer{\XXi\vdash \exists x. G}
      {\XXi'\vdash G[t/x]\qquad \someExpert{\XXi}{\XXi'}{t}}
\]
\vskip -6pt
\[
\infer{\XXi\vdash t = t}
      {\eqExpert{\XXi}}
\qquad
\infer{\XXi\vdash A}
      {\XXi'\vdash G \quad (A~\hbox{\tt :-}~G)\in\instan\Pscr
                     \quad \unfoldExpert{\XXi}{\XXi'}}
\]
% \pause
\begin{itemize}
% \item Certificates can be seen as proof terms or more in general any
%   evidence/oracle that can help, via the specification of the \textbf{expert}
%   predicates, to reconstruct a proof
\item An FPC is an instantiation of $\XXi$ and experts predicates
\end{itemize}
\end{frame}
% -----------------------
% \begin{frame}
%   \frametitle{Sample FPC 1: height bound}
%   \begin{itemize}
%   \item To begin with, we encode exhaustive generation by building proofs bounded by their
%     height (the gen strategy of $\alpha$Check)
%   % \pause
%   \item Certificates are just integers and the only active expert is a
%     simple non-zero check while backchaining: \fbox{$n\vdash G$}
%     \begin{itemize}
%     \item Note that the bound is \red{additive} wrt conjunction
%     \end{itemize}
%  \end{itemize}
%  % \pause
%  \[
% \infer{n\vdash G_1\wedge G_2}
% {n\vdash G_1\qquad n\vdash G_2 %\qquad \andExpert{\XXi}{\XXi_1}{\XXi_2}
% }
% \qquad
% \infer{n\vdash \true}
% {
%   % \trueExpert{\XXi}
% }
% \]
% \vskip -6pt
% \[
% \infer{n\vdash G_1\vee G_2}
% {n\vdash G_i%\qquad \orExpert{\XXi}{\XXi'}{i}
% }
% \qquad
% \infer{n\vdash \exists x. G}
% {n\vdash G[t/x]%\qquad \someExpert{\XXi}{\XXi'}{t}
% }
% \]
% \vskip -6pt
% \[
% \infer{n\vdash t = t}
% {
%   % \eqExpert{\XXi}
% }
% \qquad
% \infer{n + 1\vdash A}
%       {n\vdash G \quad (A~\hbox{\tt :-}~G)\in\instan\Pscr
%                      \quad n \geq 0}
% \]

% \end{frame}
% -----------------------------
\begin{frame}
  \frametitle{FPC for exhaustive generation}
  \begin{itemize}
  \item We capture exhaustive generation by building proofs bounded by their 
    size -- many others in [PPDP '19] paper
  % \pause
  \item Certificates are pairs of integers and the only active expert
    is the non-zero check while backchaining: \fbox{$n,m\vdash G$}
    %     \begin{itemize}
    % \item Note that the certificate behaves \red{multiplicatively} wrt conjunction
    % \end{itemize}

 \end{itemize}
 % \pause
 \[
\infer{n,m\vdash G_1\wedge G_2}
{n,n_1\vdash G_1\qquad n_1,m\vdash G_2 %\qquad \andExpert{\XXi}{\XXi_1}{\XXi_2}
}
\qquad
\infer{n,n\vdash \true}
{
  % \trueExpert{\XXi}
}
\]
\vskip -6pt
\[
\infer{n,m\vdash G_1\vee G_2}
{n,m\vdash G_i%\qquad \orExpert{\XXi}{\XXi'}{i}
}
\qquad
\infer{n,m\vdash \exists x. G}
{n,m\vdash G[t/x]%\qquad \someExpert{\XXi}{\XXi'}{t}
}
\]
\vskip -6pt
\[
\infer{n,n\vdash t = t}
{
  % \eqExpert{\XXi}
}
\qquad
\infer{n + 1,m\vdash A}
      {n,m\vdash G \quad (A~\hbox{\tt :-}~G)\in\instan\Pscr
                     \quad n \geq 0}
\]

\end{frame}


% -----------------------------
% \begin{frame}
%   \frametitle{Sample FPC 3: maximal info}
%   \begin{itemize}
%   \item What about recording explicitly all information within a
% proof:
% \begin{enumerate}
% \item all disjunctive choices
% \item all substitution
% instances for existential quantifiers  
% \end{enumerate}
% % \pause
%   \item Certificates are basically proof terms for Horn logic
%      \fbox{$M\vdash G$}
%     \end{itemize}

% % \pause
%  \[
% \infer{\langle M_i,M_2\rangle\vdash G_1\wedge G_2}
% {M_1\vdash G_1\qquad M_2\vdash G_2}
% \qquad
% \infer{\langle\ \rangle\vdash \true}
% {}
% \]
% \vskip -6pt
% \[
% \infer{inj_i(M)\vdash G_1\vee G_2}
% {M\vdash G_i}
% \qquad
% \infer{\langle t,M\rangle\vdash \exists x. G}
% {M\vdash G[t/x]}
% \]
% \vskip -6pt
% \[
% \infer{\mathit{refl}\vdash t = t}
% {}
% \qquad
% \infer{M\vdash A}
%       {M\vdash G \quad  (A~\hbox{\tt :-}~G)\in\instan\Pscr}                    
% \]

% \end{frame}

% % ----------------
% \begin{frame}
%   \frametitle{FPC for random generation}
%   \begin{itemize}
%   \item We support random generation by implementing \red{randomized}
%     backtracking, as opposed to the usual chronological one.
%   % \pause
%   \item There are many possible variations: the simplest one  permutes the program at each backchaining --- certificates are empty
%     \[\infer{\vdash A}
%       {\vdash G \quad  (A~\hbox{\tt :-}~G)\in\instan{\pi\Pscr}}                 
% \]
% % \pause
% \item Or we can use non-uniform distributions, by viewing certificates as assigning \textbf{weights} to clause ids
%   $$\XXi \quad\bnfas\quad\cdot\bnfalt\XXi, (id, n) $$
%   and have the backchaining expert randomically select a clause according to the weight. In QuickCheck:
%   \begin{enumerate}
%   \item the \texttt{oneof} combinator
%   \item the \texttt{frequency} combinator
%   \end{enumerate}

%   \end{itemize}
% \end{frame}

% ----------------

% \begin{frame}[fragile]
%   \frametitle{More FPC's for PBT}
%   \begin{itemize}
%   \item 
%   In previous paper we showed how FPC's reconstruct most features of PBT:
%   \begin{itemize}
%   \item random generation
%   \item \red{Pairing}  \emph{restricts} proofs
%     satisfying two FPCs simultaneously and
%  accounts for \textbf{shrinking} of (random)
%   cex:  
% \item for other proof management duties, such as bug provenance and
%   explanation
% % % \pause
% % \item The FPC way:
% %   \begin{enumerate}
% %   \item Collect all substitution terms in an existing proof (e.g., with the \textbf{Max} FPC
% %   \item Search again restricting the existential quantifiers to use
% %     (strict) subterms of terms collected in the first pass.
% %   \end{enumerate}
% \end{itemize}
% % \pause
%  \item Moves seamlessly from
%     standard first-order terms to one containing \textbf{binders}
%     (via $\lambda$-tree syntax)  with
%   $\nabla$, a formula level quantifier encoding binders mobility.
% %% \pause
% \end{itemize}
% \end{frame}
% % ----------------

% \begin{frame}[fragile]
%   \frametitle{More FPC's}
%   \begin{itemize}
%   \item FPC collecting maximal info (basically proof terms)
%   \item FPC for random generation
%   \item It is possible to build an FPC that \emph{restricts} proofs
%     satisfying two FPCs simultaneously:
%     \begin{itemize}
%     \item \red{Pairing} $\XXi_1 \oplus \XXi_2$ is  such an
%       operations on certificates% , whereby we require the corresponding
%       % experts also succeed for both $\XXi_i$ and for \lsti{orE} and
%       % \lsti{someE}, also return the same choice and substitution term
%     \end{itemize}
% % \pause
% \item For example, \fbox{$\langle (5,0), M\rangle\vdash G$} collects  the
% maximal   information of a proof $M$ of $G$ of size at most $5$.
%   % \pause
% \item Pairing also accounts for \textbf{shrinking} of (random)
%   cex:  smaller variants are generated and
%   tested to see if they are still cex.
%   \item We can use certificates also for other proof management duties, such
%   as bug provenance and explanation
% % % \pause
% % \item The FPC way:
% %   \begin{enumerate}
% %   \item Collect all substitution terms in an existing proof (e.g., with the \textbf{Max} FPC
% %   \item Search again restricting the existential quantifiers to use
% %     (strict) subterms of terms collected in the first pass.
% %   \end{enumerate}
%   \end{itemize}
% \end{frame}

% ----------------

% \begin{frame}[fragile]
%   \frametitle{From algebraic to binding signatures}
  

%   \begin{itemize}
%   \item The proof-theoretic view allows us to move seamlessly from
%     standard first-order terms to one containing \textbf{binders}
%     ($\lambda$-tree syntax)
%   \item Note: no current reasoning tool supports (I think) both proofs \textbf{and}
%     disproofs with native binders, unless via DB encodings 
% %% \pause
% \item In meta-logics based fixed points, this is accomplished with
%   $\nabla$, a formula level quantifier encoding binders mobility.
% \item $\nabla$ needs no experts

%     \end{itemize}
% \end{frame}

% --------------------------
\begin{frame}
  \frametitle{From PBT to coPBT}
  \begin{itemize}
  \item Standard PBT requires only finite computations and can be accounted for with logic programming (Horn) queries
  \item Recall failure of determinism of \emph{co-evaluation}:
    $\exists M\, V_1\, V_2\ [istm(M) \land istm( V_1) \land istm(V_2) \land M\Downarrow V_1\land M\Downarrow V_2 \land V_1 \not = V_2 ]$
    % \pause
    \item Generation (pred \texttt{istm}) is still Horn and finitary: it will be driven by FPC
      
    \item If you want to capture its
      \red{infinite} behavior of  $ M\Downarrow V_i$, you need a proof-theory with rules for
      fixed points, such as the ones underlying \emph{Abella} and
      \emph{Bedwyr}% \\
      % (there are alternatives: co-patterns, guarded recursive types, I just don't know enough)
    \item Other specs such as bisimilarity are Harrop and will also need proofs by \red{case analysis}, which is readily available with fixed points.

  \end{itemize}
\end{frame}
% --------------------------
\begin{frame}
  \frametitle{$\mu$MALL 1/2}

  \begin{itemize}
  \item Extends multiplicative additive linear logic with fixed points and free equality [Baelde \& Miller '07]:

    $\Rightarrow$ co-evaluation is encoded (using $\lambda$-tree syntax) as this greatest fixed point 
    (see linear version of Clark's completion):
    $$
\begin{small}
\begin{array}{l}
    \nu (\lambda {CE}.\lambda m.\lambda m'. (\exists M.\, m = (fun\  M) \otimes  m' = (fun\  M)  \oplus\mbox{}\\
  \quad(\exists M_1\, M_2\, M\,  V_2\, V.\ m = (app\ M_1\ M_2)  \otimes m' =  V \otimes
     ({CE} \ M_1 \ (fun\  M)) \\
  \quad \mbox{} \otimes ({CE\ } M_2\ V_2)   \otimes ({CE} \ (M \ V_2) \ V))
 \end{array}
\end{small}
 $$
\item Left sequent rule of $\nu$ is unfolding (case analysis), right is coinduction via simulation:
  \begin{small}
    \[
      \begin{array}{l}
        \infer[\nu L]{\Gamma, \nu B\vec t\vdash G}{\Gamma, B(\nu B)\vec t\vdash G}\qquad
        \infer[\nu R]{\Gamma\vdash \nu B\vec t}{\Gamma\vdash S\vec t\quad S\vec x\vdash B S\vec x}
      \end{array}
      \]
  \end{small}
  \end{itemize}
\end{frame}
% --------------------------
\begin{frame}
  \frametitle{$\mu$MALL 2/2}
  \begin{itemize}
  \item $\mu$MALL provides a proof-search interpretation for some aspects of model checking, see [Heath \& Miller '19 ] for the finite case.
%      \item $\mu$MALL has a significant and rigid focusing theorem and is sound wrt  Peano  arithmetic (which lacks focusing).
  \item Polarization: Prolog programs = least fixed points = purely positive. Dually, greatest fixed points:
    \begin{itemize}
    \item Often in model checking problems we just need arbitrary fixed points to be unrolled and polarization is neutral
    \end{itemize}
  \item Fixed points build in contraction, but for coPBT that's the only place we need it.
  \end{itemize}
\end{frame}
% ------------------------------------------------
% skip this
\begin{frame}
  \frametitle{$\mu$MALL and negation}
  \begin{itemize}
  \item Recall the PBT query: $\exists x [(\tau(x)\land P(x)) \land \neg Q(x)$
  \item With \red{negation} and logic (programming), things get hairy:
  \item In our account of PBT, we managed to see $\neg$ as negation-as-failure, no certificate needed % (just exploration of the search tree)
  % \pause
  \item $\mu$MALL builds in  for Horn spec the \red{CWA}:  $\neg Q$ as $Q\rightarrow\perp$
  \item Otherwise, we can use de Morgan and inequalities to \red{eliminate} negation. E.g., non applicative simulation $m \not\leq_a n$:
  \end{itemize}
  \begin{small}
   $$
\begin{array}{l}
 \mu(\lambda NAS.\lambda m.\lambda n.\exists M'.\,  m\Downarrow (fun\ M')
  \otimes
                         \forall N'.\, n\Downarrow (fun\ N') \multimap\mbox{}\\
  \qquad\exists R.\ ({NAS\ } (M' \ R) \  (N'\ R)))
\end{array}
$$
\end{small}
\end{frame}

\begin{frame}[fragile]
  \frametitle{A Proof-of-concept implementation: \textbf{Bedwyr}}
  We piggy-back our prototype on top of the \emph{Bedwyr} model-checker, which:
  \begin{itemize}
  \item implements a depth-first fully automatic search for focused proofs of a fragment of $\mu$MALL
  \item supports $\lambda$-tree syntax: binding in terms, pattern unification, binder mobility, $\nabla$-quantification
  \item implements (co)induction as loop detection via \textbf{tables} (modulo equivariance).
  \end{itemize}
\end{frame}


% --------------------------
\begin{frame}[fragile]
  \frametitle{A Proof-of-concept implementation: architecture}
  \begin{itemize}
  \item We use a ``multi-level'' approach where FPC drive generation and Bedwyr does the rest
  % \pause
    \item generators are reified in \lsti{prog} clauses over object
      level formulae of type \lsti{oo};
      \begin{small}
\begin{lstlisting}
Define prog : oo -> oo -> prop by
 prog (istm Ctx (app Exp1 Exp2))
    (istm Ctx Exp1) & (istm Ctx Exp2) ; ...
\end{lstlisting}
      \end{small}
      % \pause
  \item For each certificate format \lsti{cert},
    generation is driven by a meta-interpreter \lsti{check}
    parameterized by a \lsti{prog}
    \begin{small}
\begin{lstlisting}
Define check: cert -> oo -> (oo -> oo -> prop) -> prop by
 check Cert A Prog := unfoldE Cert Cert' /\ Prog A G /\ check Cert' G Prog.
\end{lstlisting}
  \end{small}

  %    % \pause


%    \end{itemize}

  \end{itemize}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Our fav example}
  Plain Bedwyr is in charge of precondition and testing phase:
      \begin{small}
\begin{lstlisting}
Define coinductive coeval : i -> i -> prop by
 coeval (fun R) (fun R) ;
 coeval (app M N) V := coeval M (fun R) /\ coeval N W /\ coeval (R W) V.

% the query: gen by height
?= check (qheight 4) (istm nl M) & (istm nl M1) & (istm nl M2) /\ coeval M M1 /\ coeval M M2 /\ (M1 = M2 -> false).
\end{lstlisting}
      \end{small}
      % \pause
      \begin{small}
\begin{lstlisting}
Found a solution  + 45ms:
 M2 = app (fun (x\ x)) (fun (x\ x))
 M1 = fun (x\ x)
 M  = app (fun (x\ app x x)) (fun (x\ app x x))
\end{lstlisting}
      \end{small}
\end{frame}


%-----------------------
  \begin{myslide}{Conclusions}
  \begin{itemize}
  \item PBT successfully
    complements theorem proving with a preliminary phase of conjecture
    testing, but its support for checking coinductive spec is unsatisfactory

\item We have shown as the FPC-based proof-theoretic reconstruction of
  PBT extends to such specs by relying on stronger logics.

   \item We have presented a proof-of-concept implementation in Bedwyr
  \end{itemize}
  
\end{myslide}


%------------------------------------------------------------
\begin{myslide}{Current and Future Work: implementation}
  \begin{itemize}
  \item PBT uses \textbf{mode} information to restrict and delay term generation:
    \begin{itemize}
    \item w.r.t.\ standard evaluation $M\Downarrow V$,  need to generate only $M$ and often can use the judgment  as a generator.
    \end{itemize}
  \item Not the case for coinductive judgments that typically do not compute, only check (see also bisimilarity etc.)
    \begin{itemize}
    \item refuting non-det of \texttt{coeval} requires the unbounded, orthogonal generation of 3 terms. Does not scale.
    \end{itemize}
    % \pause
  \item deal with the combinatorial explosion via \red{fuzzying}:
    \begin{itemize}
    \item Generate one term and obtain the others by (random)
      mutations, possibly preserving global invariants (viz.,
      typing) %(see statesin non-interefernce)
    \end{itemize}
  \item Pair it with the idea of \emph{pre-computing} equivalence
    classes of terms of given depth 'a la Tarau ['18]
  \end{itemize}
  \end{myslide}
%------------------------------------------------------------
\begin{myslide}{Current and Future Work: extensions}
  \begin{itemize}
  \item Integrate with Abella's workflow, both at the top-level
    (disproving conjectures) and inside a proof attempt (disproving
    subgoals).    
\item  Investigate an \emph{explanation} tool for attributing ``blame'' for
the origin of a counterexample
% \begin{enumerate}
% \item certificate distillation for wrong answers
% \item abduction for missing answers
% \end{enumerate}
  \item Look into sub-structural object logics for PBT-ing specs about state and concurrency
  \end{itemize}
\end{myslide}
\begin{frame}
  \begin{center}
    \begin{LARGE}
      Thank you!
    \end{LARGE}
  \end{center}

\end{frame}

\end{document}
% --------------------------------------------------------------

% JUNK


  \newcommand{\wedgep}{\wedge^{\!+}}
\begin{myslide}{$\mu$MALL}
  \begin{itemize}
  \item As the plan is to have a PBT tool for \textbf{Abella}, we
    have in mind specs and checks in \red{multiplicative additive linear
      logic} with (for the time being) \red{least fixed points}
    (Baelde \& Miller)
  \item E.g.~, the \texttt{append} predicate is:
\begin{align*}
\plus \equiv &\mu\lambda A\lambda xs\lambda ys\lambda zs~(xs={\bf nl}\wedgep ys=zs)~\vee\\
&\exists x'\exists xs'\exists zs'(xs=\mathbf{cns}~x'~xs'\wedgep zs=\mathbf{cns}~x'~zs'\wedgep A~xs'~ys~zs')
\end{align*}
\item Usual \red{polarization} for LP: everything is \red{positive}
  --- note, no atoms.
\item Searching for a cex is searching for a proof of a formula like
  \(\exists x\colon\tau\ [P(x) \wedgep \neg Q(x)]\)
  is a single \red{bipole} --- a positive phase followed by a negative
  one.
\item Correspond to the intuition that generation is hard, testing a deterministic computation
  \end{itemize}
\end{myslide}

%--------------------------------------
%--------------------------------------
\begin{frame}[fragile]
  \frametitle{FPC for the common man}
  \begin{itemize}
  \item We defined certificates for families of proofs (the generation
    phase) limited either by the number of inference rules that they
    contain, by their size, or by both.
\item They essentially translate into meta-interpreters that perform bounded generation, not only of terms but of derivations.
\item As a proof of concept, we implement this in $\lambda$Prolog and
  we use \NF to implement negation --- it's a shortcut, but theoretically, think fixed point and negation as $A\rightarrow\perp$.

\item We use the two-level approach: OL specs are encoded as
  \texttt{prog} clauses and a \texttt{check} predicates will
  meta-interpret them using the size/height certificates to guide the
  generation.
\item Checking $\forall x \oftp elt, \forall xs, ys\oftp eltlist\ [\mathit{rev\ xs\ ys \rightarrow xs = ys }] $ is
  \begin{small}
\begin{verbatim}
cexrev Xs Ys :- 
   check (qgen (qheight 3)) (is_eltlist Xs), % generate
   solve (rev Xs Ys), not (Xs = Ys).         % test

\end{verbatim}
  \end{small}
  \end{itemize}
\end{frame}
%--------------------------------------

%-----------------------------------  

%--------------------------------------
\begin{myslide}{Future Work: more case studies}
  \begin{itemize}
  \item Search for deeper known bugs
    \begin{itemize}
    \item ``value'' restriction in ML with references and let-polymorphous
    \item intersection types with computational effects
    \end{itemize}
  \item Search for unknown bugs in ($\lambda$)Prolog code ``in the
    wild'' (e.g.\ Hannan's ``Extended natural semantics'' or even old
    CENTAUR stuff)
  \item Tackle \red{coinductive} specs, to look for
    \begin{itemize}
    \item Two process that are similar but \textbf{not} bisimilar
    \item $\lambda$-terms that are ground- but \textbf{not} applicative-bisimilar\dots
    \end{itemize}
   \red{Tabling}  could prove handy.
  \item Implement \red{random} generators e.g.\ with an
    \texttt{unfold} expert that may flip a coin when selecting a
    clause to backchain on.
  \end{itemize}
\end{myslide}

%--------------------------------------
\begin{myslide}{Future Work: architecture}
  \begin{itemize}
  \item Integrate with Abella's workflow, both at the top-level
    (disproving conjectures) and inside a proof attempt (disproving
    subgoals).

  \item Long-ish time view: a mini Sledgehammer protocol for Abella,
    by which conjectures are under the hood PB-tested: if no cex
    reported \red{proof outlines} are used to try and conclude the
    proof. 

    \item Keeping in mind that Abella's implementation
      \textbf{not} immediately meant for search
    \item 
  Previous attempts with FPC
      kernels with primitive $\nabla$ written as inductive definition in
      Abella proper seems too slow for generation
  \end{itemize}
\end{myslide}

%--------------------------------------
\begin{frame}
  \frametitle{The blame game}
  \begin{itemize}
  \item Suppose your PBT tool reports a cex. Now what? You're not
    getting payed just for finding faults\dots
  \item Staring at a potentially huge spec even with a cex in hand not
    the best way to go. Two issues:
    \begin{enumerate}
    \item Soundness: your spec is plain wrong and returns an answer that should not hold
    \item Completeness: you've forgotten to encode some info and some answers are not produced.
    \end{enumerate}
  \item FPC to the rescue (possibly):
    \begin{enumerate}
    \item Use certificate \red{distillation} to restrict to a more manageable set of suspects
    \item Use \red{abductive} experts to collect sets of assumptions that should hold but don't
    \end{enumerate}
  \end{itemize}
\end{frame}
%--------------------------------------
\begin{frame}
  \begin{center}
    \begin{LARGE}
      Thanks!
    \end{LARGE}
  \end{center}

\end{frame}
\end{document}






%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:

%  LocalWords:  Blanco LFMTP Twelf Coq Abella ehm PBT QuickCheck ICFP Momigliano PBT's wrt coinductively combinator al QuickChick's LazySmallCheck Renaud SmallCheck pred DM Heyting reified Ctx qheight det combinatorial fuzzying subgoals OL
%  LocalWords:  Claessen Haskell QuickCheck's FsCheck pre QC's Quis
%  LocalWords:  custodiet ipsos custodes SUT Coq's cex shrinkers Agda
%  LocalWords:  subgoal PVS QuickChick PLT Redex Spoofax backchaining
%  LocalWords:  llcl ilist hd tl instantiations intensional MMT xs ys
%  LocalWords:  Baelde zs nl cns bipole FPC Foundational clllll arg
%  LocalWords:  pos eval lookup reachability bisimulation NEs neq bg
%  LocalWords:  bisimilarity disunification clllllll prog TAPL LF et
%  LocalWords:  documemtation POPLMark Findler atomatically DFS nabla
%  LocalWords:  datatypes Subsumption equivariant Prolog Hannah's ish
%  LocalWords:  coinductive bisimilar applicative Bedwyr backchain FP
%  LocalWords:  abductive elt eltlist Hannan's PPDP focusable grnd oo
%  LocalWords:  ing Erlang corecursion Clarks multiplicatively interp
%  LocalWords:  coPBT CBV Grall Filinski Ancona PCF surjective Tarau
%  LocalWords:  FPC's istm Harrop NAS fav Chihani Peano equivariance
%  LocalWords:  CWA
